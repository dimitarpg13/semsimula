{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addition Game: Dynamic Programming Solution\n",
        "\n",
        "## Game Definition (from Blackwell's \"Game Theory and Statistical Decisions\")\n",
        "\n",
        "> *The parameters of the game k and N are given. Player I and Player II alternately choose integers, each choice being one of the integers 1,...,k and each choice made with the knowledge of all preceding choices. As soon as the sum of the chosen integers exceeds N, the last player to choose loses the game and pays his opponent one unit.*\n",
        "\n",
        "In this notebook, we solve the Addition game using **Dynamic Programming** (DP). Unlike sampling-based methods (Monte Carlo, Q-learning), DP computes optimal values and policies exactly by solving the **Bellman equations** directly. This is possible because the game has:\n",
        "- A finite state space (sums 0 to N+k)\n",
        "- Known transition dynamics (deterministic)\n",
        "- Complete model of the game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['font.family'] = 'monospace'\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['axes.facecolor'] = '#1e1e2e'\n",
        "plt.rcParams['figure.facecolor'] = '#11111b'\n",
        "plt.rcParams['axes.edgecolor'] = '#6c7086'\n",
        "plt.rcParams['axes.labelcolor'] = '#89b4fa'\n",
        "plt.rcParams['xtick.color'] = '#a6adc8'\n",
        "plt.rcParams['ytick.color'] = '#a6adc8'\n",
        "plt.rcParams['text.color'] = '#cdd6f4'\n",
        "plt.rcParams['grid.color'] = '#313244'\n",
        "plt.rcParams['grid.alpha'] = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bellman Equations for Two-Player Zero-Sum Games\n",
        "\n",
        "For a two-player zero-sum game, we define the **minimax value** V(s, player) as the expected payoff to Player I when both players play optimally from state s.\n",
        "\n",
        "### State Representation\n",
        "- State s = (current_sum, current_player)\n",
        "- current_sum ∈ {0, 1, ..., N+k}\n",
        "- current_player ∈ {0 (Player I), 1 (Player II)}\n",
        "\n",
        "### Terminal States\n",
        "When sum > N, the player who just moved loses:\n",
        "- V(s > N, just_moved=I) = -1 (Player I just lost)\n",
        "- V(s > N, just_moved=II) = +1 (Player II just lost, so Player I wins)\n",
        "\n",
        "### Bellman Optimality Equations\n",
        "\n",
        "**For Player I (maximizer):**\n",
        "$$V(s, \\text{Player I}) = \\max_{a \\in \\{1,...,k\\}} V(s+a, \\text{Player II})$$\n",
        "\n",
        "**For Player II (minimizer):**\n",
        "$$V(s, \\text{Player II}) = \\min_{a \\in \\{1,...,k\\}} V(s+a, \\text{Player I})$$\n",
        "\n",
        "These equations express the minimax principle: Player I maximizes, Player II minimizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Game Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GameState:\n",
        "    \"\"\"Represents the current state of the Addition game.\"\"\"\n",
        "    current_sum: int\n",
        "    current_player: int  # 0 for Player I, 1 for Player II\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash((self.current_sum, self.current_player))\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return (self.current_sum == other.current_sum and \n",
        "                self.current_player == other.current_player)\n",
        "\n",
        "\n",
        "class AdditionGame:\n",
        "    \"\"\"The Addition game environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self) -> GameState:\n",
        "        \"\"\"Reset the game to initial state.\"\"\"\n",
        "        self.state = GameState(current_sum=0, current_player=0)\n",
        "        self.history: List[int] = []\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.state\n",
        "    \n",
        "    def get_valid_actions(self) -> List[int]:\n",
        "        \"\"\"Returns list of valid actions (1 to k).\"\"\"\n",
        "        return list(range(1, self.k + 1))\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[GameState, float, float, bool]:\n",
        "        \"\"\"Execute an action and return (new_state, reward_p1, reward_p2, done).\"\"\"\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game is already over!\")\n",
        "        \n",
        "        self.history.append(action)\n",
        "        current_player = self.state.current_player\n",
        "        new_sum = self.state.current_sum + action\n",
        "        \n",
        "        if new_sum > self.N:\n",
        "            self.done = True\n",
        "            self.winner = 1 - current_player\n",
        "            reward_p1 = 1.0 if self.winner == 0 else -1.0\n",
        "            reward_p2 = -reward_p1\n",
        "        else:\n",
        "            reward_p1 = 0.0\n",
        "            reward_p2 = 0.0\n",
        "        \n",
        "        self.state = GameState(\n",
        "            current_sum=new_sum,\n",
        "            current_player=1 - current_player\n",
        "        )\n",
        "        \n",
        "        return self.state, reward_p1, reward_p2, self.done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Value Iteration Algorithm\n",
        "\n",
        "**Value Iteration** solves the Bellman equations iteratively until convergence:\n",
        "\n",
        "1. Initialize V(s) = 0 for all states\n",
        "2. Repeat until convergence:\n",
        "   - For each state s where it's Player I's turn: V(s) ← max_a V(s+a)\n",
        "   - For each state s where it's Player II's turn: V(s) ← min_a V(s+a)\n",
        "3. Extract optimal policy from converged values\n",
        "\n",
        "For the Addition game, we can solve this exactly with **backward induction** since the game has a natural ordering (higher sums come \"later\" in the game).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DynamicProgrammingSolver:\n",
        "    \"\"\"Solves the Addition game using Dynamic Programming (Value Iteration).\n",
        "    \n",
        "    Computes optimal values V(s) and optimal policies for both players\n",
        "    by solving the Bellman equations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        \n",
        "        # Value function: V[sum][player] = value to Player I\n",
        "        # We need values for sums 0 to N+k (terminal states)\n",
        "        self.V = np.zeros((N + k + 1, 2))\n",
        "        \n",
        "        # Q-values: Q[sum][player][action] = value\n",
        "        self.Q = np.zeros((N + k + 1, 2, k))\n",
        "        \n",
        "        # Optimal policy: policy[sum][player] = optimal action\n",
        "        self.policy = np.zeros((N + 1, 2), dtype=int)\n",
        "        \n",
        "        # Initialize terminal values\n",
        "        self._init_terminal_values()\n",
        "        \n",
        "    def _init_terminal_values(self):\n",
        "        \"\"\"Initialize values for terminal states (sum > N).\"\"\"\n",
        "        # When sum > N, the player who just moved loses\n",
        "        # V is always from Player I's perspective\n",
        "        for s in range(self.N + 1, self.N + self.k + 1):\n",
        "            # If it's Player I's turn at s > N, that means Player II just moved\n",
        "            # and caused sum to exceed N, so Player II loses, Player I wins\n",
        "            self.V[s, 0] = +1  # Player I's turn at terminal -> Player I wins\n",
        "            \n",
        "            # If it's Player II's turn at s > N, Player I just caused overflow\n",
        "            # so Player I loses\n",
        "            self.V[s, 1] = -1  # Player II's turn at terminal -> Player I loses\n",
        "    \n",
        "    def solve(self, verbose: bool = True) -> dict:\n",
        "        \"\"\"Solve the game using backward induction (a form of Value Iteration).\n",
        "        \n",
        "        Since sum always increases, we can solve exactly by working backwards\n",
        "        from terminal states.\n",
        "        \"\"\"\n",
        "        iterations = 0\n",
        "        \n",
        "        # Process states from highest sum to lowest (backward induction)\n",
        "        for s in range(self.N, -1, -1):\n",
        "            iterations += 1\n",
        "            \n",
        "            for player in [0, 1]:  # 0 = Player I, 1 = Player II\n",
        "                best_value = None\n",
        "                best_action = None\n",
        "                \n",
        "                for a_idx, a in enumerate(range(1, self.k + 1)):\n",
        "                    next_sum = s + a\n",
        "                    next_player = 1 - player\n",
        "                    \n",
        "                    # Get value of next state\n",
        "                    if next_sum > self.N:\n",
        "                        # Terminal: current player loses\n",
        "                        if player == 0:  # Player I moves and loses\n",
        "                            value = -1\n",
        "                        else:  # Player II moves and loses, Player I wins\n",
        "                            value = +1\n",
        "                    else:\n",
        "                        value = self.V[next_sum, next_player]\n",
        "                    \n",
        "                    # Store Q-value\n",
        "                    self.Q[s, player, a_idx] = value\n",
        "                    \n",
        "                    # Update best action based on player's objective\n",
        "                    if best_value is None:\n",
        "                        best_value = value\n",
        "                        best_action = a\n",
        "                    elif player == 0:  # Player I maximizes\n",
        "                        if value > best_value:\n",
        "                            best_value = value\n",
        "                            best_action = a\n",
        "                    else:  # Player II minimizes\n",
        "                        if value < best_value:\n",
        "                            best_value = value\n",
        "                            best_action = a\n",
        "                \n",
        "                self.V[s, player] = best_value\n",
        "                self.policy[s, player] = best_action\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Dynamic Programming solved in {iterations} iterations\")\n",
        "            print(f\"Game value (V(0, Player I)): {self.V[0, 0]:.2f}\")\n",
        "            if self.V[0, 0] > 0:\n",
        "                print(\"Player I wins with optimal play!\")\n",
        "            elif self.V[0, 0] < 0:\n",
        "                print(\"Player II wins with optimal play!\")\n",
        "            else:\n",
        "                print(\"Game is a draw with optimal play!\")\n",
        "        \n",
        "        return {\n",
        "            'V': self.V.copy(),\n",
        "            'Q': self.Q.copy(),\n",
        "            'policy': self.policy.copy(),\n",
        "            'game_value': self.V[0, 0]\n",
        "        }\n",
        "    \n",
        "    def get_optimal_action(self, state: GameState) -> int:\n",
        "        \"\"\"Get optimal action for given state.\"\"\"\n",
        "        if state.current_sum > self.N:\n",
        "            return 1  # Game is over, return dummy\n",
        "        return self.policy[state.current_sum, state.current_player]\n",
        "    \n",
        "    def get_value(self, state: GameState) -> float:\n",
        "        \"\"\"Get value of state from Player I's perspective.\"\"\"\n",
        "        return self.V[min(state.current_sum, self.N + self.k), state.current_player]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Solving the Game\n",
        "\n",
        "Let's solve the Addition game with parameters `k=3` and `N=10`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing the Value Function and Optimal Policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_dp_solution(solver: DynamicProgrammingSolver, k: int, N: int):\n",
        "    \"\"\"Visualize the DP solution: values, Q-values, and optimal policy.\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    fig.suptitle('Dynamic Programming Solution for Addition Game', \n",
        "                 fontsize=16, fontweight='bold', color='#89b4fa')\n",
        "    \n",
        "    sums = list(range(0, N + 1))\n",
        "    actions = list(range(1, k + 1))\n",
        "    \n",
        "    # --- Row 1: Player I's perspective ---\n",
        "    \n",
        "    # Value function for Player I\n",
        "    ax1 = axes[0, 0]\n",
        "    values_p1 = [solver.V[s, 0] for s in sums]\n",
        "    colors = ['#a6e3a1' if v > 0 else '#f38ba8' if v < 0 else '#f9e2af' for v in values_p1]\n",
        "    ax1.bar(sums, values_p1, color=colors, edgecolor='white', linewidth=0.5)\n",
        "    ax1.axhline(y=0, color='#6c7086', linewidth=1)\n",
        "    ax1.set_xlabel('Current Sum')\n",
        "    ax1.set_ylabel('Value')\n",
        "    ax1.set_title('V(s, Player I) - Value Function', color='#89b4fa')\n",
        "    ax1.set_ylim(-1.2, 1.2)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Q-values for Player I\n",
        "    ax2 = axes[0, 1]\n",
        "    q_matrix_p1 = solver.Q[:N+1, 0, :]\n",
        "    im2 = ax2.imshow(q_matrix_p1, aspect='auto', cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "    ax2.set_yticks(range(len(sums)))\n",
        "    ax2.set_yticklabels(sums)\n",
        "    ax2.set_xticks(range(k))\n",
        "    ax2.set_xticklabels(actions)\n",
        "    ax2.set_ylabel('Current Sum')\n",
        "    ax2.set_xlabel('Action')\n",
        "    ax2.set_title('Q(s, a, Player I)', color='#89b4fa')\n",
        "    plt.colorbar(im2, ax=ax2, label='Q-value')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(k):\n",
        "            color = 'white' if abs(q_matrix_p1[i, j]) > 0.5 else 'black'\n",
        "            ax2.text(j, i, f'{q_matrix_p1[i, j]:.0f}', ha='center', va='center', \n",
        "                     color=color, fontsize=9, fontweight='bold')\n",
        "    \n",
        "    # Optimal policy for Player I\n",
        "    ax3 = axes[0, 2]\n",
        "    policy_p1 = solver.policy[:, 0]\n",
        "    colors_policy = ['#f38ba8', '#f9e2af', '#a6e3a1']\n",
        "    ax3.barh(sums, policy_p1, color=[colors_policy[a-1] for a in policy_p1], \n",
        "             edgecolor='white', linewidth=0.5)\n",
        "    ax3.set_xlabel('Optimal Action')\n",
        "    ax3.set_ylabel('Current Sum')\n",
        "    ax3.set_title('Optimal Policy π*(s, Player I)', color='#89b4fa')\n",
        "    ax3.set_xticks([1, 2, 3])\n",
        "    ax3.set_xlim(0, k + 0.5)\n",
        "    ax3.invert_yaxis()\n",
        "    \n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=colors_policy[i], label=f'Action {i+1}') for i in range(k)]\n",
        "    ax3.legend(handles=legend_elements, loc='lower right', facecolor='#1e1e2e', edgecolor='#6c7086')\n",
        "    \n",
        "    # --- Row 2: Player II's perspective ---\n",
        "    \n",
        "    # Value function for Player II's turn (still from P1's perspective)\n",
        "    ax4 = axes[1, 0]\n",
        "    values_p2 = [solver.V[s, 1] for s in sums]\n",
        "    colors = ['#a6e3a1' if v > 0 else '#f38ba8' if v < 0 else '#f9e2af' for v in values_p2]\n",
        "    ax4.bar(sums, values_p2, color=colors, edgecolor='white', linewidth=0.5)\n",
        "    ax4.axhline(y=0, color='#6c7086', linewidth=1)\n",
        "    ax4.set_xlabel('Current Sum')\n",
        "    ax4.set_ylabel('Value')\n",
        "    ax4.set_title('V(s, Player II) - Value when P2 moves', color='#89b4fa')\n",
        "    ax4.set_ylim(-1.2, 1.2)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Q-values for Player II\n",
        "    ax5 = axes[1, 1]\n",
        "    q_matrix_p2 = solver.Q[:N+1, 1, :]\n",
        "    im5 = ax5.imshow(q_matrix_p2, aspect='auto', cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "    ax5.set_yticks(range(len(sums)))\n",
        "    ax5.set_yticklabels(sums)\n",
        "    ax5.set_xticks(range(k))\n",
        "    ax5.set_xticklabels(actions)\n",
        "    ax5.set_ylabel('Current Sum')\n",
        "    ax5.set_xlabel('Action')\n",
        "    ax5.set_title('Q(s, a, Player II)', color='#89b4fa')\n",
        "    plt.colorbar(im5, ax=ax5, label='Q-value')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(k):\n",
        "            color = 'white' if abs(q_matrix_p2[i, j]) > 0.5 else 'black'\n",
        "            ax5.text(j, i, f'{q_matrix_p2[i, j]:.0f}', ha='center', va='center', \n",
        "                     color=color, fontsize=9, fontweight='bold')\n",
        "    \n",
        "    # Optimal policy for Player II\n",
        "    ax6 = axes[1, 2]\n",
        "    policy_p2 = solver.policy[:, 1]\n",
        "    ax6.barh(sums, policy_p2, color=[colors_policy[a-1] for a in policy_p2], \n",
        "             edgecolor='white', linewidth=0.5)\n",
        "    ax6.set_xlabel('Optimal Action')\n",
        "    ax6.set_ylabel('Current Sum')\n",
        "    ax6.set_title('Optimal Policy π*(s, Player II)', color='#89b4fa')\n",
        "    ax6.set_xticks([1, 2, 3])\n",
        "    ax6.set_xlim(0, k + 0.5)\n",
        "    ax6.invert_yaxis()\n",
        "    ax6.legend(handles=legend_elements, loc='lower right', facecolor='#1e1e2e', edgecolor='#6c7086')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_dp_solution(solver, K, N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Explicit Bellman Equations\n",
        "\n",
        "Let's write out the Bellman equations for each state explicitly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_bellman_equations(solver: DynamicProgrammingSolver, k: int, N: int):\n",
        "    \"\"\"Print the Bellman equations for each state.\"\"\"\n",
        "    \n",
        "    print(\"BELLMAN EQUATIONS FOR THE ADDITION GAME\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Parameters: k={k}, N={N}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"TERMINAL STATES (sum > N):\")\n",
        "    print(\"-\"*70)\n",
        "    print(\"  V(s > N, Player I's turn) = +1  (Player II just lost)\")\n",
        "    print(\"  V(s > N, Player II's turn) = -1 (Player I just lost)\")\n",
        "    print()\n",
        "    \n",
        "    print(\"PLAYER I's BELLMAN EQUATIONS (maximizing):\")\n",
        "    print(\"-\"*70)\n",
        "    for s in range(N, -1, -1):\n",
        "        q_terms = []\n",
        "        for a in range(1, k + 1):\n",
        "            next_s = s + a\n",
        "            if next_s > N:\n",
        "                q_terms.append(f\"Q({s},{a})=-1\")\n",
        "            else:\n",
        "                q_terms.append(f\"Q({s},{a})=V({next_s},P2)={solver.V[next_s,1]:+.0f}\")\n",
        "        \n",
        "        opt_a = solver.policy[s, 0]\n",
        "        v_val = solver.V[s, 0]\n",
        "        print(f\"  V({s:2d}, P1) = max{{ {', '.join(q_terms)} }} = {v_val:+.0f}  [π*={opt_a}]\")\n",
        "    \n",
        "    print()\n",
        "    print(\"PLAYER II's BELLMAN EQUATIONS (minimizing):\")\n",
        "    print(\"-\"*70)\n",
        "    for s in range(N, -1, -1):\n",
        "        q_terms = []\n",
        "        for a in range(1, k + 1):\n",
        "            next_s = s + a\n",
        "            if next_s > N:\n",
        "                q_terms.append(f\"Q({s},{a})=+1\")\n",
        "            else:\n",
        "                q_terms.append(f\"Q({s},{a})=V({next_s},P1)={solver.V[next_s,0]:+.0f}\")\n",
        "        \n",
        "        opt_a = solver.policy[s, 1]\n",
        "        v_val = solver.V[s, 1]\n",
        "        print(f\"  V({s:2d}, P2) = min{{ {', '.join(q_terms)} }} = {v_val:+.0f}  [π*={opt_a}]\")\n",
        "\n",
        "print_bellman_equations(solver, K, N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dynamic Programming Agent\n",
        "\n",
        "Let's create agents that use the DP-computed optimal policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DPAgent:\n",
        "    \"\"\"An agent that plays using the DP-computed optimal policy.\"\"\"\n",
        "    \n",
        "    def __init__(self, player_id: int, solver: DynamicProgrammingSolver):\n",
        "        self.player_id = player_id\n",
        "        self.solver = solver\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = False) -> int:\n",
        "        \"\"\"Select the optimal action computed by DP.\"\"\"\n",
        "        return self.solver.get_optimal_action(state)\n",
        "    \n",
        "    def get_value(self, state: GameState) -> float:\n",
        "        \"\"\"Get the value of a state.\"\"\"\n",
        "        return self.solver.get_value(state)\n",
        "\n",
        "\n",
        "# Create DP agents\n",
        "dp_agent1 = DPAgent(player_id=0, solver=solver)\n",
        "dp_agent2 = DPAgent(player_id=1, solver=solver)\n",
        "\n",
        "print(\"DP agents created using optimal policies from Value Iteration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Watching Optimal Play\n",
        "\n",
        "Let's watch the DP agents play a game, showing the value function at each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_game_with_values(game: AdditionGame, agent1: DPAgent, agent2: DPAgent):\n",
        "    \"\"\"Play a game showing values at each step.\"\"\"\n",
        "    state = game.reset()\n",
        "    agents = [agent1, agent2]\n",
        "    names = [\"Player I\", \"Player II\"]\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"OPTIMAL PLAY: Addition Game with k={game.k}, N={game.N}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Game Value V(0, P1) = {agent1.get_value(state):+.0f}\")\n",
        "    print(\"This means Player I \" + (\"WINS\" if agent1.get_value(state) > 0 else \"LOSES\") + \" with optimal play!\")\n",
        "    print(\"=\"*70)\n",
        "    print()\n",
        "    \n",
        "    move = 1\n",
        "    while not game.done:\n",
        "        player = state.current_player\n",
        "        agent = agents[player]\n",
        "        name = names[player]\n",
        "        \n",
        "        # Get all Q-values for this state\n",
        "        q_values = []\n",
        "        for a in range(1, game.k + 1):\n",
        "            next_sum = state.current_sum + a\n",
        "            if next_sum > game.N:\n",
        "                q_val = -1 if player == 0 else +1\n",
        "            else:\n",
        "                q_val = solver.V[next_sum, 1 - player]\n",
        "            q_values.append(f\"Q(a={a})={q_val:+.0f}\")\n",
        "        \n",
        "        action = agent.select_action(state, game.get_valid_actions())\n",
        "        value = agent.get_value(state)\n",
        "        \n",
        "        print(f\"Move {move}: {name}'s turn at sum={state.current_sum}\")\n",
        "        print(f\"         V(s={state.current_sum}, {name}) = {value:+.0f}\")\n",
        "        print(f\"         Q-values: {', '.join(q_values)}\")\n",
        "        print(f\"         {'Maximizing' if player == 0 else 'Minimizing'} -> chooses action {action}\")\n",
        "        print(f\"         Sum: {state.current_sum} -> {state.current_sum + action}\")\n",
        "        \n",
        "        state, _, _, done = game.step(action)\n",
        "        \n",
        "        if done:\n",
        "            print(f\"\\n         *** SUM ({state.current_sum}) EXCEEDS {game.N}! ***\")\n",
        "            winner = \"Player I\" if game.winner == 0 else \"Player II\"\n",
        "            print(f\"\\n  WINNER: {winner}\")\n",
        "        else:\n",
        "            print()\n",
        "        \n",
        "        move += 1\n",
        "    \n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Game history: {game.history}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "play_game_with_values(game, dp_agent1, dp_agent2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analyzing Different Game Parameters\n",
        "\n",
        "Let's use DP to solve various game configurations and see the pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_games_with_dp():\n",
        "    \"\"\"Solve multiple game configurations using DP.\"\"\"\n",
        "    \n",
        "    print(\"DYNAMIC PROGRAMMING ANALYSIS OF DIFFERENT GAME CONFIGURATIONS\")\n",
        "    print(\"=\"*75)\n",
        "    print(f\"{'k':>3} {'N':>4} {'States':>8} {'V(0,P1)':>10} {'Winner':>12} {'P1 Policy[0]':>13}\")\n",
        "    print(\"-\"*75)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    configs = [\n",
        "        (2, 5), (2, 6), (2, 7), (2, 8), (2, 9),\n",
        "        (3, 8), (3, 9), (3, 10), (3, 11), (3, 12),\n",
        "        (4, 12), (4, 13), (4, 14), (4, 15), (4, 16),\n",
        "        (5, 18), (5, 19), (5, 20), (5, 21),\n",
        "    ]\n",
        "    \n",
        "    for k, n in configs:\n",
        "        solver = DynamicProgrammingSolver(k=k, N=n)\n",
        "        solver.solve(verbose=False)\n",
        "        \n",
        "        game_value = solver.V[0, 0]\n",
        "        winner = \"Player I\" if game_value > 0 else \"Player II\" if game_value < 0 else \"Draw\"\n",
        "        states = 2 * (n + 1)\n",
        "        first_action = solver.policy[0, 0]\n",
        "        \n",
        "        # Check if it matches modular arithmetic theory\n",
        "        # Losing positions: s ≡ N (mod k+1)\n",
        "        # P1 wins iff 0 is NOT a losing position, i.e., N mod (k+1) != 0\n",
        "        theory_p1_wins = (n % (k + 1)) != 0\n",
        "        theory_matches = (game_value > 0) == theory_p1_wins\n",
        "        \n",
        "        mark = \"✓\" if theory_matches else \"✗\"\n",
        "        \n",
        "        print(f\"{k:>3} {n:>4} {states:>8} {game_value:>+10.0f} {winner:>12} {first_action:>13} {mark}\")\n",
        "        \n",
        "        results.append({\n",
        "            'k': k, 'N': n, 'value': game_value, 'winner': winner,\n",
        "            'first_action': first_action, 'theory_matches': theory_matches\n",
        "        })\n",
        "    \n",
        "    print(\"-\"*75)\n",
        "    print(\"Theory: Player I wins iff N mod (k+1) ≠ 0\")\n",
        "    print(\"✓ = DP result matches theoretical prediction\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "results = analyze_games_with_dp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Comparison: DP vs Learning Methods\n",
        "\n",
        "Dynamic Programming computes exact optimal solutions, but requires a complete model of the environment. Let's compare the characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_methods():\n",
        "    \"\"\"Compare DP with learning-based methods.\"\"\"\n",
        "    \n",
        "    comparison = \"\"\"\n",
        "    ╔═══════════════════════════════════════════════════════════════════════════════╗\n",
        "    ║              COMPARISON: DYNAMIC PROGRAMMING vs LEARNING METHODS              ║\n",
        "    ╠═══════════════════════════════════════════════════════════════════════════════╣\n",
        "    ║ Aspect              │ Dynamic Programming │ Q-Learning/MC/GB                  ║\n",
        "    ╠═══════════════════════════════════════════════════════════════════════════════╣\n",
        "    ║ Model Required      │ Yes (complete)      │ No (model-free)                   ║\n",
        "    ║ Computation         │ Polynomial in |S|   │ Depends on # episodes             ║\n",
        "    ║ Solution Quality    │ Exact optimal       │ Approximate (converges)           ║\n",
        "    ║ Sample Efficiency   │ Perfect (no samples)│ Requires many episodes            ║\n",
        "    ║ Exploration         │ Not needed          │ Critical (ε-greedy, etc.)         ║\n",
        "    ║ Online Learning     │ No                  │ Yes                               ║\n",
        "    ║ Large State Spaces  │ Intractable         │ Can use function approximation    ║\n",
        "    ║ Stochastic Games    │ Needs probabilities │ Learns from samples               ║\n",
        "    ╚═══════════════════════════════════════════════════════════════════════════════╝\n",
        "    \"\"\"\n",
        "    print(comparison)\n",
        "    \n",
        "    print(\"\\nFor the Addition Game specifically:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"State space size: 2 × (N+1) = {2*(N+1)} states\")\n",
        "    print(f\"DP solves in: O(N × k) = O({N} × {K}) = {N*K} operations\")\n",
        "    print(\"Learning methods: Need ~50,000 episodes to converge\")\n",
        "    print()\n",
        "    print(\"DP is clearly superior for this small, fully-known game.\")\n",
        "    print(\"But learning methods generalize to unknown/large environments!\")\n",
        "\n",
        "compare_methods()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Game parameters\n",
        "K = 3  # Can choose 1, 2, or 3\n",
        "N = 10  # Game ends when sum exceeds 10\n",
        "\n",
        "# Create game and solver\n",
        "game = AdditionGame(k=K, N=N)\n",
        "solver = DynamicProgrammingSolver(k=K, N=N)\n",
        "\n",
        "print(f\"Solving Addition game with k={K}, N={N}\")\n",
        "print(f\"State space: {N + 1} non-terminal states × 2 players = {2*(N+1)} states\")\n",
        "print(f\"Action space: {K} actions per state\")\n",
        "print()\n",
        "\n",
        "# Solve using Dynamic Programming\n",
        "result = solver.solve(verbose=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
