{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addition Game: Adversarial Reinforcement Learning\n",
        "\n",
        "## Game Definition (from Blackwell's \"Game Theory and Statistical Decisions\")\n",
        "\n",
        "> *The parameters of the game k and N are given. Player I and Player II alternately choose integers, each choice being one of the integers 1,...,k and each choice made with the knowledge of all preceding choices. As soon as the sum of the chosen integers exceeds N, the last player to choose loses the game and pays his opponent one unit.*\n",
        "\n",
        "This is a perfect information, zero-sum, sequential game. We'll implement Q-learning agents that learn to play optimally through self-play.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['font.family'] = 'monospace'\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['axes.facecolor'] = '#1a1a2e'\n",
        "plt.rcParams['figure.facecolor'] = '#0f0f23'\n",
        "plt.rcParams['axes.edgecolor'] = '#4a4a6a'\n",
        "plt.rcParams['axes.labelcolor'] = '#e0e0e0'\n",
        "plt.rcParams['xtick.color'] = '#a0a0a0'\n",
        "plt.rcParams['ytick.color'] = '#a0a0a0'\n",
        "plt.rcParams['text.color'] = '#e0e0e0'\n",
        "plt.rcParams['grid.color'] = '#2a2a4a'\n",
        "plt.rcParams['grid.alpha'] = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Game Environment\n",
        "\n",
        "The state of the game is fully characterized by:\n",
        "- The current sum of all chosen integers\n",
        "- Whose turn it is (Player 1 or Player 2)\n",
        "\n",
        "The game ends when the sum exceeds N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GameState:\n",
        "    \"\"\"Represents the current state of the Addition game.\"\"\"\n",
        "    current_sum: int\n",
        "    current_player: int  # 0 for Player I, 1 for Player II\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash((self.current_sum, self.current_player))\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return (self.current_sum == other.current_sum and \n",
        "                self.current_player == other.current_player)\n",
        "\n",
        "\n",
        "class AdditionGame:\n",
        "    \"\"\"The Addition game environment.\n",
        "    \n",
        "    Parameters:\n",
        "        k: Maximum integer that can be chosen (choices are 1, 2, ..., k)\n",
        "        N: Threshold - the game ends when sum exceeds N\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self) -> GameState:\n",
        "        \"\"\"Reset the game to initial state.\"\"\"\n",
        "        self.state = GameState(current_sum=0, current_player=0)\n",
        "        self.history: List[int] = []\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.state\n",
        "    \n",
        "    def get_valid_actions(self) -> List[int]:\n",
        "        \"\"\"Returns list of valid actions (1 to k).\"\"\"\n",
        "        return list(range(1, self.k + 1))\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[GameState, float, float, bool]:\n",
        "        \"\"\"Execute an action and return (new_state, reward_p1, reward_p2, done).\n",
        "        \n",
        "        Returns:\n",
        "            new_state: The resulting game state\n",
        "            reward_p1: Reward for Player I (+1 win, -1 loss, 0 ongoing)\n",
        "            reward_p2: Reward for Player II (+1 win, -1 loss, 0 ongoing)\n",
        "            done: Whether the game has ended\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game is already over!\")\n",
        "        \n",
        "        if action < 1 or action > self.k:\n",
        "            raise ValueError(f\"Invalid action {action}. Must be in [1, {self.k}]\")\n",
        "        \n",
        "        # Record action\n",
        "        self.history.append(action)\n",
        "        current_player = self.state.current_player\n",
        "        \n",
        "        # Update sum\n",
        "        new_sum = self.state.current_sum + action\n",
        "        \n",
        "        # Check if game ends\n",
        "        if new_sum > self.N:\n",
        "            self.done = True\n",
        "            # Current player loses (they made sum exceed N)\n",
        "            self.winner = 1 - current_player\n",
        "            reward_p1 = 1.0 if self.winner == 0 else -1.0\n",
        "            reward_p2 = -reward_p1\n",
        "        else:\n",
        "            reward_p1 = 0.0\n",
        "            reward_p2 = 0.0\n",
        "        \n",
        "        # Update state\n",
        "        self.state = GameState(\n",
        "            current_sum=new_sum,\n",
        "            current_player=1 - current_player  # Switch player\n",
        "        )\n",
        "        \n",
        "        return self.state, reward_p1, reward_p2, self.done\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Print the current game state.\"\"\"\n",
        "        print(f\"Sum: {self.state.current_sum} / N={self.N}\")\n",
        "        print(f\"History: {self.history}\")\n",
        "        print(f\"Current player: {'I' if self.state.current_player == 0 else 'II'}\")\n",
        "        if self.done:\n",
        "            print(f\"Winner: Player {'I' if self.winner == 0 else 'II'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Q-Learning Agent\n",
        "\n",
        "We'll use tabular Q-learning since the state space is finite and manageable. Each agent maintains a Q-table mapping (state, action) pairs to expected returns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Q-learning agent for the Addition game.\n",
        "    \n",
        "    Uses Îµ-greedy exploration with decaying Îµ.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        player_id: int,\n",
        "        k: int,\n",
        "        learning_rate: float = 0.1,\n",
        "        discount_factor: float = 0.99,\n",
        "        epsilon_start: float = 1.0,\n",
        "        epsilon_end: float = 0.01,\n",
        "        epsilon_decay: float = 0.9995\n",
        "    ):\n",
        "        self.player_id = player_id\n",
        "        self.k = k\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        \n",
        "        # Q-table: maps (sum, action) -> Q-value\n",
        "        # We only need to store states where it's this agent's turn\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        \n",
        "        # Statistics\n",
        "        self.wins = 0\n",
        "        self.losses = 0\n",
        "        self.games = 0\n",
        "    \n",
        "    def get_state_key(self, state: GameState) -> int:\n",
        "        \"\"\"Convert state to key for Q-table lookup.\"\"\"\n",
        "        return state.current_sum\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = True) -> int:\n",
        "        \"\"\"Select action using Îµ-greedy policy.\"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "        \n",
        "        state_key = self.get_state_key(state)\n",
        "        q_values = self.q_table[state_key]\n",
        "        \n",
        "        # Find action with highest Q-value\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        \n",
        "        for action in valid_actions:\n",
        "            value = q_values[action]\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "        \n",
        "        # If all Q-values are 0 (unvisited), choose randomly\n",
        "        if best_action is None or best_value == 0:\n",
        "            return random.choice(valid_actions)\n",
        "        \n",
        "        return best_action\n",
        "    \n",
        "    def update(self, state: GameState, action: int, reward: float, \n",
        "               next_state: Optional[GameState], done: bool, next_valid_actions: List[int]):\n",
        "        \"\"\"Update Q-value using the Q-learning update rule.\"\"\"\n",
        "        state_key = self.get_state_key(state)\n",
        "        current_q = self.q_table[state_key][action]\n",
        "        \n",
        "        if done:\n",
        "            # Terminal state: no future rewards\n",
        "            target = reward\n",
        "        else:\n",
        "            # For the Addition game, the next state where this agent acts\n",
        "            # is after the opponent moves. We need to account for this.\n",
        "            # Here, we use the immediate reward + discounted future.\n",
        "            # Since it's a two-player game, we'll handle this in the training loop.\n",
        "            next_state_key = self.get_state_key(next_state)\n",
        "            next_q_values = self.q_table[next_state_key]\n",
        "            max_next_q = max([next_q_values[a] for a in next_valid_actions], default=0)\n",
        "            target = reward + self.gamma * max_next_q\n",
        "        \n",
        "        # Q-learning update\n",
        "        self.q_table[state_key][action] = current_q + self.lr * (target - current_q)\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    def record_result(self, won: bool):\n",
        "        \"\"\"Record game result.\"\"\"\n",
        "        self.games += 1\n",
        "        if won:\n",
        "            self.wins += 1\n",
        "        else:\n",
        "            self.losses += 1\n",
        "    \n",
        "    def win_rate(self) -> float:\n",
        "        \"\"\"Calculate win rate.\"\"\"\n",
        "        return self.wins / max(1, self.games)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Loop\n",
        "\n",
        "We train both agents through self-play. Each episode is a full game, and both agents learn from the outcomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agents(\n",
        "    game: AdditionGame,\n",
        "    agent1: QLearningAgent,\n",
        "    agent2: QLearningAgent,\n",
        "    num_episodes: int = 50000,\n",
        "    log_interval: int = 1000\n",
        ") -> dict:\n",
        "    \"\"\"Train two agents through self-play.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing training statistics\n",
        "    \"\"\"\n",
        "    agents = [agent1, agent2]\n",
        "    stats = {\n",
        "        'episodes': [],\n",
        "        'agent1_win_rate': [],\n",
        "        'agent2_win_rate': [],\n",
        "        'epsilon': [],\n",
        "        'avg_game_length': []\n",
        "    }\n",
        "    \n",
        "    game_lengths = []\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
        "        state = game.reset()\n",
        "        \n",
        "        # Store transitions for each agent\n",
        "        transitions = [[], []]  # [agent1_transitions, agent2_transitions]\n",
        "        \n",
        "        while not game.done:\n",
        "            current_player = state.current_player\n",
        "            agent = agents[current_player]\n",
        "            \n",
        "            valid_actions = game.get_valid_actions()\n",
        "            action = agent.select_action(state, valid_actions)\n",
        "            \n",
        "            # Store state and action for later update\n",
        "            transitions[current_player].append({\n",
        "                'state': state,\n",
        "                'action': action\n",
        "            })\n",
        "            \n",
        "            next_state, reward_p1, reward_p2, done = game.step(action)\n",
        "            state = next_state\n",
        "        \n",
        "        # Game ended - assign terminal rewards\n",
        "        rewards = [reward_p1, reward_p2]\n",
        "        game_lengths.append(len(game.history))\n",
        "        \n",
        "        # Update Q-values for each agent using backward updates\n",
        "        for player_id in [0, 1]:\n",
        "            agent = agents[player_id]\n",
        "            player_transitions = transitions[player_id]\n",
        "            reward = rewards[player_id]\n",
        "            \n",
        "            # Update from last state backward\n",
        "            for i in reversed(range(len(player_transitions))):\n",
        "                trans = player_transitions[i]\n",
        "                \n",
        "                if i == len(player_transitions) - 1:\n",
        "                    # Last action by this agent\n",
        "                    agent.update(\n",
        "                        trans['state'], \n",
        "                        trans['action'], \n",
        "                        reward,\n",
        "                        None, \n",
        "                        True,\n",
        "                        game.get_valid_actions()\n",
        "                    )\n",
        "                else:\n",
        "                    # Intermediate action - next state is the next time this agent acts\n",
        "                    next_trans = player_transitions[i + 1]\n",
        "                    agent.update(\n",
        "                        trans['state'],\n",
        "                        trans['action'],\n",
        "                        0,  # No intermediate reward\n",
        "                        next_trans['state'],\n",
        "                        False,\n",
        "                        game.get_valid_actions()\n",
        "                    )\n",
        "            \n",
        "            # Record result\n",
        "            agent.record_result(game.winner == player_id)\n",
        "        \n",
        "        # Decay epsilon for both agents\n",
        "        agent1.decay_epsilon()\n",
        "        agent2.decay_epsilon()\n",
        "        \n",
        "        # Log statistics\n",
        "        if (episode + 1) % log_interval == 0:\n",
        "            stats['episodes'].append(episode + 1)\n",
        "            stats['agent1_win_rate'].append(agent1.wins / max(1, agent1.games) * 100)\n",
        "            stats['agent2_win_rate'].append(agent2.wins / max(1, agent2.games) * 100)\n",
        "            stats['epsilon'].append(agent1.epsilon)\n",
        "            stats['avg_game_length'].append(np.mean(game_lengths[-log_interval:]))\n",
        "            \n",
        "            # Reset counters for windowed stats\n",
        "            agent1.wins = agent1.losses = agent1.games = 0\n",
        "            agent2.wins = agent2.losses = agent2.games = 0\n",
        "    \n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the Agents\n",
        "\n",
        "Let's train two Q-learning agents on the Addition game with parameters `k=3` and `N=10`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Game parameters\n",
        "K = 3  # Can choose 1, 2, or 3\n",
        "N = 10  # Game ends when sum exceeds 10\n",
        "\n",
        "# Create game and agents\n",
        "game = AdditionGame(k=K, N=N)\n",
        "\n",
        "agent1 = QLearningAgent(\n",
        "    player_id=0,\n",
        "    k=K,\n",
        "    learning_rate=0.2,\n",
        "    discount_factor=0.95,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay=0.9998\n",
        ")\n",
        "\n",
        "agent2 = QLearningAgent(\n",
        "    player_id=1,\n",
        "    k=K,\n",
        "    learning_rate=0.2,\n",
        "    discount_factor=0.95,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay=0.9998\n",
        ")\n",
        "\n",
        "print(f\"Training agents on Addition game with k={K}, N={N}\")\n",
        "print(f\"Player I starts, choices are integers from 1 to {K}\")\n",
        "print(f\"First player to make sum exceed {N} loses\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agents\n",
        "stats = train_agents(game, agent1, agent2, num_episodes=50000, log_interval=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Addition Game: Q-Learning Training Progress', fontsize=16, fontweight='bold', color='#00d4aa')\n",
        "\n",
        "# Win rates\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(stats['episodes'], stats['agent1_win_rate'], color='#ff6b6b', linewidth=2, label='Player I')\n",
        "ax1.plot(stats['episodes'], stats['agent2_win_rate'], color='#4ecdc4', linewidth=2, label='Player II')\n",
        "ax1.axhline(y=50, color='#666', linestyle='--', alpha=0.5, label='50% baseline')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Win Rate (%)')\n",
        "ax1.set_title('Win Rates Over Training', color='#00d4aa')\n",
        "ax1.legend(facecolor='#1a1a2e', edgecolor='#4a4a6a')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Epsilon decay\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(stats['episodes'], stats['epsilon'], color='#ffd93d', linewidth=2)\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Epsilon (Îµ)')\n",
        "ax2.set_title('Exploration Rate Decay', color='#00d4aa')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Average game length\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(stats['episodes'], stats['avg_game_length'], color='#c9b1ff', linewidth=2)\n",
        "ax3.set_xlabel('Episode')\n",
        "ax3.set_ylabel('Average Game Length (moves)')\n",
        "ax3.set_title('Average Game Length', color='#00d4aa')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Win rate difference (advantage)\n",
        "ax4 = axes[1, 1]\n",
        "advantage = [a1 - a2 for a1, a2 in zip(stats['agent1_win_rate'], stats['agent2_win_rate'])]\n",
        "colors = ['#ff6b6b' if a > 0 else '#4ecdc4' for a in advantage]\n",
        "ax4.bar(stats['episodes'], advantage, color=colors, alpha=0.7, width=400)\n",
        "ax4.axhline(y=0, color='#fff', linewidth=1)\n",
        "ax4.set_xlabel('Episode')\n",
        "ax4.set_ylabel('Win Rate Difference (%)')\n",
        "ax4.set_title('Player I Advantage (positive = Player I favored)', color='#00d4aa')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyzing Learned Strategies\n",
        "\n",
        "Let's examine the Q-values learned by each agent to understand their strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_policy(agent: QLearningAgent, k: int, N: int, player_name: str):\n",
        "    \"\"\"Visualize the learned policy as a heatmap of preferred actions.\"\"\"\n",
        "    \n",
        "    # For each possible sum (state), find the preferred action\n",
        "    sums = list(range(0, N + 1))\n",
        "    actions = list(range(1, k + 1))\n",
        "    \n",
        "    # Create Q-value matrix\n",
        "    q_matrix = np.zeros((len(sums), len(actions)))\n",
        "    \n",
        "    for i, s in enumerate(sums):\n",
        "        q_values = agent.q_table[s]\n",
        "        for j, a in enumerate(actions):\n",
        "            q_matrix[i, j] = q_values[a]\n",
        "    \n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    fig.suptitle(f'{player_name} Learned Strategy', fontsize=14, fontweight='bold', color='#00d4aa')\n",
        "    \n",
        "    # Q-value heatmap\n",
        "    im1 = ax1.imshow(q_matrix, aspect='auto', cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "    ax1.set_yticks(range(len(sums)))\n",
        "    ax1.set_yticklabels(sums)\n",
        "    ax1.set_xticks(range(len(actions)))\n",
        "    ax1.set_xticklabels(actions)\n",
        "    ax1.set_ylabel('Current Sum')\n",
        "    ax1.set_xlabel('Action (number to add)')\n",
        "    ax1.set_title('Q-Values', color='#00d4aa')\n",
        "    plt.colorbar(im1, ax=ax1, label='Q-value')\n",
        "    \n",
        "    # Add best action annotations\n",
        "    for i, s in enumerate(sums):\n",
        "        best_action = np.argmax(q_matrix[i]) + 1\n",
        "        for j, a in enumerate(actions):\n",
        "            color = 'white' if abs(q_matrix[i, j]) > 0.5 else 'black'\n",
        "            weight = 'bold' if a == best_action else 'normal'\n",
        "            ax1.text(j, i, f'{q_matrix[i, j]:.2f}', ha='center', va='center', \n",
        "                     color=color, fontsize=8, fontweight=weight)\n",
        "    \n",
        "    # Best action visualization\n",
        "    best_actions = np.argmax(q_matrix, axis=1) + 1\n",
        "    colors = ['#ff6b6b', '#ffd93d', '#4ecdc4']\n",
        "    \n",
        "    bars = ax2.barh(sums, best_actions, color=[colors[a-1] for a in best_actions], edgecolor='white', linewidth=0.5)\n",
        "    ax2.set_xlabel('Optimal Action')\n",
        "    ax2.set_ylabel('Current Sum')\n",
        "    ax2.set_title('Learned Optimal Actions', color='#00d4aa')\n",
        "    ax2.set_xticks([1, 2, 3])\n",
        "    ax2.set_xlim(0, k + 0.5)\n",
        "    ax2.invert_yaxis()\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=colors[i], label=f'Action {i+1}') for i in range(k)]\n",
        "    ax2.legend(handles=legend_elements, loc='lower right', facecolor='#1a1a2e', edgecolor='#4a4a6a')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return best_actions\n",
        "\n",
        "print(\"Player I Strategy (moves first):\")\n",
        "p1_policy = visualize_policy(agent1, K, N, \"Player I\")\n",
        "\n",
        "print(\"\\nPlayer II Strategy:\")\n",
        "p2_policy = visualize_policy(agent2, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Theoretical Optimal Strategy\n",
        "\n",
        "For the Addition game with parameters `k` and `N`, the optimal strategy is known:\n",
        "\n",
        "- **Losing positions** (for the player to move): Sums of the form `N - m(k+1)` for `m = 0, 1, 2, ...`\n",
        "- **Winning positions**: All other sums\n",
        "\n",
        "The optimal play from a winning position is to choose an action that puts the opponent in a losing position.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def compute_optimal_strategy(k: int, N: int):\n",
        "    \"\"\"Compute the theoretically optimal strategy for the Addition game.\n",
        "    \n",
        "    Key insight: A position (sum) is a LOSING position for the player to move\n",
        "    if and only if sum â‰¡ N (mod k+1). From any other position, the player\n",
        "    can force the opponent into a losing position.\n",
        "    \n",
        "    Returns:\n",
        "        losing_positions: List of sums that are losing for the player to move\n",
        "        optimal_actions: Dict mapping sum -> optimal action\n",
        "    \"\"\"\n",
        "    # Losing positions: sums where sum â‰¡ N (mod k+1)\n",
        "    # These are: N, N-(k+1), N-2(k+1), ...\n",
        "    losing_positions = []\n",
        "    s = N\n",
        "    while s >= 0:\n",
        "        losing_positions.append(s)\n",
        "        s -= (k + 1)\n",
        "    losing_positions = sorted(losing_positions)\n",
        "    \n",
        "    # Compute optimal actions from each position\n",
        "    optimal_actions = {}\n",
        "    for s in range(0, N + 1):\n",
        "        if s in losing_positions:\n",
        "            # In a losing position, any move loses (opponent can force win)\n",
        "            # Just pick the move that delays loss longest\n",
        "            optimal_actions[s] = 1  # Convention: play smallest\n",
        "        else:\n",
        "            # Find action that puts opponent in losing position\n",
        "            for a in range(1, k + 1):\n",
        "                next_sum = s + a\n",
        "                if next_sum in losing_positions or next_sum > N:\n",
        "                    # If next_sum > N, we lose immediately - avoid if possible\n",
        "                    if next_sum <= N:\n",
        "                        optimal_actions[s] = a\n",
        "                        break\n",
        "            else:\n",
        "                # Shouldn't happen in theory, but handle gracefully\n",
        "                # Pick action that gets closest to a losing position\n",
        "                for a in range(1, k + 1):\n",
        "                    optimal_actions[s] = a\n",
        "                    break\n",
        "    \n",
        "    return losing_positions, optimal_actions\n",
        "\n",
        "# Compute optimal strategy for our game\n",
        "losing_positions, optimal_actions = compute_optimal_strategy(K, N)\n",
        "\n",
        "print(f\"Game parameters: k={K}, N={N}\")\n",
        "print(f\"\\nLosing positions (for the player to move): {losing_positions}\")\n",
        "print(f\"These satisfy: sum â‰¡ {N} (mod {K+1})\")\n",
        "\n",
        "print(f\"\\nOptimal actions from each position:\")\n",
        "for s in range(0, N + 1):\n",
        "    is_losing = \"LOSING\" if s in losing_positions else \"winning\"\n",
        "    print(f\"  Sum {s:2d}: play {optimal_actions[s]} ({is_losing})\")\n",
        "\n",
        "# Determine who wins with optimal play\n",
        "first_position_type = \"LOSING\" if 0 in losing_positions else \"winning\"\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Initial sum is 0, which is a {first_position_type} position.\")\n",
        "if 0 in losing_positions:\n",
        "    print(\"â†’ With optimal play, Player II wins!\")\n",
        "else:\n",
        "    print(\"â†’ With optimal play, Player I wins!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def compare_learned_vs_optimal(agent: QLearningAgent, optimal_actions: dict, k: int, N: int, player_name: str):\n",
        "    \"\"\"Compare learned policy with theoretically optimal policy.\"\"\"\n",
        "    matches = 0\n",
        "    mismatches = []\n",
        "    \n",
        "    for s in range(0, N + 1):\n",
        "        # Get learned action (greedy)\n",
        "        q_values = agent.q_table[s]\n",
        "        if q_values:\n",
        "            learned_action = max(range(1, k + 1), key=lambda a: q_values[a])\n",
        "        else:\n",
        "            learned_action = 1  # Default\n",
        "        \n",
        "        optimal_action = optimal_actions[s]\n",
        "        \n",
        "        if learned_action == optimal_action:\n",
        "            matches += 1\n",
        "        else:\n",
        "            # Check if the learned action is also optimal (multiple optimal actions possible)\n",
        "            # An action is optimal if it reaches a losing position for opponent\n",
        "            next_sum_learned = s + learned_action\n",
        "            next_sum_optimal = s + optimal_action\n",
        "            \n",
        "            # Both are optimal if they reach the same type of position\n",
        "            both_win = (next_sum_learned in losing_positions) and (next_sum_optimal in losing_positions)\n",
        "            both_ok = (next_sum_learned > N and next_sum_optimal > N)  # Both lose, doesn't matter\n",
        "            \n",
        "            if both_win or both_ok:\n",
        "                matches += 1\n",
        "            else:\n",
        "                mismatches.append((s, learned_action, optimal_action))\n",
        "    \n",
        "    accuracy = matches / (N + 1) * 100\n",
        "    \n",
        "    print(f\"\\n{player_name} Policy Comparison:\")\n",
        "    print(f\"  Accuracy: {accuracy:.1f}% ({matches}/{N + 1} positions)\")\n",
        "    \n",
        "    if mismatches:\n",
        "        print(f\"  Mismatches:\")\n",
        "        for s, learned, optimal in mismatches[:5]:  # Show first 5\n",
        "            print(f\"    Sum {s}: learned={learned}, optimal={optimal}\")\n",
        "        if len(mismatches) > 5:\n",
        "            print(f\"    ... and {len(mismatches) - 5} more\")\n",
        "    else:\n",
        "        print(\"  Perfect match with optimal strategy! âœ“\")\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# Compare both agents\n",
        "acc1 = compare_learned_vs_optimal(agent1, optimal_actions, K, N, \"Player I\")\n",
        "acc2 = compare_learned_vs_optimal(agent2, optimal_actions, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation: Learned Agents vs Optimal Play\n",
        "\n",
        "Let's test our trained agents against an optimal player to see how well they've learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgent:\n",
        "    \"\"\"An agent that plays the theoretically optimal strategy.\"\"\"\n",
        "    \n",
        "    def __init__(self, player_id: int, k: int, N: int):\n",
        "        self.player_id = player_id\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        _, self.optimal_actions = compute_optimal_strategy(k, N)\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = False) -> int:\n",
        "        \"\"\"Always play the optimal action.\"\"\"\n",
        "        return self.optimal_actions.get(state.current_sum, 1)\n",
        "\n",
        "\n",
        "def evaluate_agents(game: AdditionGame, agent1, agent2, num_games: int = 1000, verbose: bool = False):\n",
        "    \"\"\"Evaluate two agents by playing multiple games.\"\"\"\n",
        "    wins = [0, 0]\n",
        "    \n",
        "    for _ in range(num_games):\n",
        "        state = game.reset()\n",
        "        agents = [agent1, agent2]\n",
        "        \n",
        "        while not game.done:\n",
        "            current_player = state.current_player\n",
        "            agent = agents[current_player]\n",
        "            \n",
        "            valid_actions = game.get_valid_actions()\n",
        "            action = agent.select_action(state, valid_actions, training=False)\n",
        "            state, _, _, _ = game.step(action)\n",
        "        \n",
        "        wins[game.winner] += 1\n",
        "    \n",
        "    return wins[0], wins[1]\n",
        "\n",
        "\n",
        "# Create an optimal agent\n",
        "optimal_agent = OptimalAgent(player_id=0, k=K, N=N)\n",
        "\n",
        "print(\"Evaluation Results (1000 games each):\\n\")\n",
        "\n",
        "# Test 1: Learned Agent 1 (as Player I) vs Optimal Agent (as Player II)\n",
        "print(\"Test 1: Learned Player I vs Optimal Player II\")\n",
        "w1, w2 = evaluate_agents(game, agent1, optimal_agent, num_games=1000)\n",
        "print(f\"  Learned wins: {w1}, Optimal wins: {w2}\")\n",
        "print(f\"  Learned win rate: {w1/10:.1f}%\")\n",
        "\n",
        "# Test 2: Optimal Agent (as Player I) vs Learned Agent 2 (as Player II)  \n",
        "print(\"\\nTest 2: Optimal Player I vs Learned Player II\")\n",
        "w1, w2 = evaluate_agents(game, optimal_agent, agent2, num_games=1000)\n",
        "print(f\"  Optimal wins: {w1}, Learned wins: {w2}\")\n",
        "print(f\"  Learned win rate: {w2/10:.1f}%\")\n",
        "\n",
        "# Test 3: Learned vs Learned\n",
        "print(\"\\nTest 3: Learned Player I vs Learned Player II (self-play)\")\n",
        "w1, w2 = evaluate_agents(game, agent1, agent2, num_games=1000)\n",
        "print(f\"  Player I wins: {w1}, Player II wins: {w2}\")\n",
        "print(f\"  Player I win rate: {w1/10:.1f}%\")\n",
        "\n",
        "# Theoretical expectation\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Theoretical expectation with k=3, N=10:\")\n",
        "print(\"  0 is not a losing position (0 mod 4 â‰  2)\")\n",
        "print(\"  â†’ With optimal play from both sides, Player I should ALWAYS win!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Watching a Sample Game\n",
        "\n",
        "Let's watch the trained agents play a game step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_game_verbose(game: AdditionGame, agent1, agent2, agent1_name: str = \"Agent 1\", agent2_name: str = \"Agent 2\"):\n",
        "    \"\"\"Play a game and print each move.\"\"\"\n",
        "    state = game.reset()\n",
        "    agents = [agent1, agent2]\n",
        "    names = [agent1_name, agent2_name]\n",
        "    \n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ADDITION GAME: k={game.k}, N={game.N}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"{agent1_name} (Player I) vs {agent2_name} (Player II)\")\n",
        "    print(f\"First to make sum exceed {game.N} loses!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    move_num = 1\n",
        "    while not game.done:\n",
        "        current_player = state.current_player\n",
        "        agent = agents[current_player]\n",
        "        name = names[current_player]\n",
        "        \n",
        "        valid_actions = game.get_valid_actions()\n",
        "        action = agent.select_action(state, valid_actions, training=False)\n",
        "        \n",
        "        print(f\"Move {move_num}: {name} chooses {action}\")\n",
        "        print(f\"         Sum: {state.current_sum} â†’ {state.current_sum + action}\")\n",
        "        \n",
        "        state, _, _, done = game.step(action)\n",
        "        \n",
        "        if done:\n",
        "            print(f\"\\n         *** SUM ({state.current_sum}) EXCEEDS {game.N}! ***\")\n",
        "            print(f\"\\nðŸ† WINNER: {names[game.winner]} (Player {'I' if game.winner == 0 else 'II'})\")\n",
        "        else:\n",
        "            print()\n",
        "        \n",
        "        move_num += 1\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Game history: {game.history}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "# Watch a game between the two learned agents\n",
        "print(\"Game 1: Learned agents playing each other\\n\")\n",
        "play_game_verbose(game, agent1, agent2, \"Learned-I\", \"Learned-II\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Watch a game between learned and optimal\n",
        "print(\"Game 2: Learned Player I vs Optimal Player II\\n\")\n",
        "play_game_verbose(game, agent1, optimal_agent, \"Learned-I\", \"Optimal-II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Experimenting with Different Parameters\n",
        "\n",
        "Let's see how the game dynamics change with different values of k and N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_game(k: int, N: int):\n",
        "    \"\"\"Analyze a game configuration and determine the theoretical winner.\"\"\"\n",
        "    losing_positions, _ = compute_optimal_strategy(k, N)\n",
        "    \n",
        "    # Check if initial position (sum=0) is losing for Player I\n",
        "    player1_starts_losing = 0 in losing_positions\n",
        "    theoretical_winner = \"Player II\" if player1_starts_losing else \"Player I\"\n",
        "    \n",
        "    return {\n",
        "        'k': k,\n",
        "        'N': N,\n",
        "        'losing_positions': losing_positions,\n",
        "        'theoretical_winner': theoretical_winner,\n",
        "        'modulus': k + 1,\n",
        "        'N_mod_k1': N % (k + 1)\n",
        "    }\n",
        "\n",
        "# Analyze different game configurations\n",
        "print(\"Analysis of Different Game Configurations\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'k':>3} {'N':>4} {'mod':>4} {'N mod(k+1)':>10} {'Losing Positions':>25} {'Winner':>12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "configs = [\n",
        "    (2, 7), (2, 8), (2, 9),    # k=2\n",
        "    (3, 9), (3, 10), (3, 11),  # k=3\n",
        "    (4, 15), (4, 16), (4, 17), # k=4\n",
        "    (5, 20), (5, 21),          # k=5\n",
        "]\n",
        "\n",
        "for k, N in configs:\n",
        "    result = analyze_game(k, N)\n",
        "    losing_str = str(result['losing_positions'])\n",
        "    if len(losing_str) > 25:\n",
        "        losing_str = losing_str[:22] + \"...\"\n",
        "    print(f\"{k:>3} {N:>4} {result['modulus']:>4} {result['N_mod_k1']:>10} {losing_str:>25} {result['theoretical_winner']:>12}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"\\nKey insight: Player I wins iff 0 is NOT a losing position,\")\n",
        "print(\"             which happens iff N mod (k+1) â‰  0.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "In this notebook, we implemented the **Addition game** from Blackwell's \"Game Theory and Statistical Decisions\" and trained two Q-learning agents through adversarial self-play.\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Game Theory Analysis**: The Addition game has a deterministic optimal strategy based on modular arithmetic. The losing positions for the player to move are those where the current sum satisfies `sum â‰¡ N (mod k+1)`.\n",
        "\n",
        "2. **RL Learning**: Through self-play with Îµ-greedy exploration, both Q-learning agents were able to discover strategies that closely approximate the theoretically optimal play.\n",
        "\n",
        "3. **First-Mover Advantage**: For most game configurations (when `N mod (k+1) â‰  0`), Player I has a winning strategy. The RL agents learned to exploit this structural advantage.\n",
        "\n",
        "### Connections to Blackwell's Work\n",
        "\n",
        "This game is a classic example of a **perfect information, zero-sum, two-player game** with:\n",
        "- Complete observability (both players know the full history)\n",
        "- Deterministic transitions (no randomness in the game mechanics)\n",
        "- A clear minimax solution\n",
        "\n",
        "Blackwell used such games to illustrate fundamental concepts in game theory and decision theory. The existence of optimal strategies in these games follows from the backward induction principle (Zermelo's theorem).\n",
        "\n",
        "### Extensions\n",
        "\n",
        "Possible extensions to this work include:\n",
        "- Using deep Q-networks for larger state spaces\n",
        "- Implementing other adversarial learning algorithms (MCTS, AlphaZero-style)\n",
        "- Analyzing convergence rates and sample complexity\n",
        "- Extending to stochastic or imperfect information variants\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
