{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addition Game: Gradient Bandit Agents\n",
        "\n",
        "## Game Definition (from Blackwell's \"Game Theory and Statistical Decisions\")\n",
        "\n",
        "> *The parameters of the game k and N are given. Player I and Player II alternately choose integers, each choice being one of the integers 1,...,k and each choice made with the knowledge of all preceding choices. As soon as the sum of the chosen integers exceeds N, the last player to choose loses the game and pays his opponent one unit.*\n",
        "\n",
        "In this notebook, we implement **Gradient Bandit** agents that learn to play through self-play. Unlike Q-learning which estimates action values directly, gradient bandits learn a *preference* for each action and use softmax to convert preferences to probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['font.family'] = 'monospace'\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['axes.facecolor'] = '#16213e'\n",
        "plt.rcParams['figure.facecolor'] = '#0f0f23'\n",
        "plt.rcParams['axes.edgecolor'] = '#4a4a6a'\n",
        "plt.rcParams['axes.labelcolor'] = '#e94560'\n",
        "plt.rcParams['xtick.color'] = '#a0a0a0'\n",
        "plt.rcParams['ytick.color'] = '#a0a0a0'\n",
        "plt.rcParams['text.color'] = '#e0e0e0'\n",
        "plt.rcParams['grid.color'] = '#1a1a3a'\n",
        "plt.rcParams['grid.alpha'] = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Game Environment\n",
        "\n",
        "The state of the game is characterized by:\n",
        "- The current sum of all chosen integers\n",
        "- Whose turn it is (Player I or Player II)\n",
        "\n",
        "The game ends when the sum exceeds N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GameState:\n",
        "    \"\"\"Represents the current state of the Addition game.\"\"\"\n",
        "    current_sum: int\n",
        "    current_player: int  # 0 for Player I, 1 for Player II\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash((self.current_sum, self.current_player))\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return (self.current_sum == other.current_sum and \n",
        "                self.current_player == other.current_player)\n",
        "\n",
        "\n",
        "class AdditionGame:\n",
        "    \"\"\"The Addition game environment.\n",
        "    \n",
        "    Parameters:\n",
        "        k: Maximum integer that can be chosen (choices are 1, 2, ..., k)\n",
        "        N: Threshold - the game ends when sum exceeds N\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self) -> GameState:\n",
        "        \"\"\"Reset the game to initial state.\"\"\"\n",
        "        self.state = GameState(current_sum=0, current_player=0)\n",
        "        self.history: List[int] = []\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.state\n",
        "    \n",
        "    def get_valid_actions(self) -> List[int]:\n",
        "        \"\"\"Returns list of valid actions (1 to k).\"\"\"\n",
        "        return list(range(1, self.k + 1))\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[GameState, float, float, bool]:\n",
        "        \"\"\"Execute an action and return (new_state, reward_p1, reward_p2, done).\"\"\"\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game is already over!\")\n",
        "        \n",
        "        if action < 1 or action > self.k:\n",
        "            raise ValueError(f\"Invalid action {action}. Must be in [1, {self.k}]\")\n",
        "        \n",
        "        self.history.append(action)\n",
        "        current_player = self.state.current_player\n",
        "        new_sum = self.state.current_sum + action\n",
        "        \n",
        "        if new_sum > self.N:\n",
        "            self.done = True\n",
        "            self.winner = 1 - current_player\n",
        "            reward_p1 = 1.0 if self.winner == 0 else -1.0\n",
        "            reward_p2 = -reward_p1\n",
        "        else:\n",
        "            reward_p1 = 0.0\n",
        "            reward_p2 = 0.0\n",
        "        \n",
        "        self.state = GameState(\n",
        "            current_sum=new_sum,\n",
        "            current_player=1 - current_player\n",
        "        )\n",
        "        \n",
        "        return self.state, reward_p1, reward_p2, self.done\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Print the current game state.\"\"\"\n",
        "        print(f\"Sum: {self.state.current_sum} / N={self.N}\")\n",
        "        print(f\"History: {self.history}\")\n",
        "        print(f\"Current player: {'I' if self.state.current_player == 0 else 'II'}\")\n",
        "        if self.done:\n",
        "            print(f\"Winner: Player {'I' if self.winner == 0 else 'II'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Gradient Bandit Algorithm\n",
        "\n",
        "The **Gradient Bandit** algorithm learns a numerical *preference* $H_t(a)$ for each action. Actions are selected according to a softmax distribution:\n",
        "\n",
        "$$\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k} e^{H_t(b)}}$$\n",
        "\n",
        "After receiving reward $R_t$, preferences are updated using stochastic gradient ascent:\n",
        "\n",
        "$$H_{t+1}(A_t) = H_t(A_t) + \\alpha (R_t - \\bar{R}_t)(1 - \\pi_t(A_t))$$\n",
        "$$H_{t+1}(a) = H_t(a) - \\alpha (R_t - \\bar{R}_t)\\pi_t(a) \\quad \\text{for } a \\neq A_t$$\n",
        "\n",
        "where $\\bar{R}_t$ is a baseline (typically the average reward so far).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradientBanditAgent:\n",
        "    \"\"\"Gradient Bandit agent for the Addition game.\n",
        "    \n",
        "    Uses softmax action selection with preference-based learning.\n",
        "    Each state has its own set of action preferences.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        player_id: int,\n",
        "        k: int,\n",
        "        alpha: float = 0.1,\n",
        "        use_baseline: bool = True,\n",
        "        temperature: float = 1.0\n",
        "    ):\n",
        "        self.player_id = player_id\n",
        "        self.k = k\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.use_baseline = use_baseline\n",
        "        self.temperature = temperature\n",
        "        \n",
        "        # Preferences H(s, a) for each state-action pair\n",
        "        # Initialized to 0 (uniform softmax initially)\n",
        "        self.preferences: Dict[int, np.ndarray] = defaultdict(\n",
        "            lambda: np.zeros(k)\n",
        "        )\n",
        "        \n",
        "        # Baseline: average reward per state\n",
        "        self.reward_baseline: Dict[int, float] = defaultdict(float)\n",
        "        self.state_visits: Dict[int, int] = defaultdict(int)\n",
        "        \n",
        "        # Statistics\n",
        "        self.wins = 0\n",
        "        self.losses = 0\n",
        "        self.games = 0\n",
        "    \n",
        "    def softmax(self, preferences: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute softmax probabilities from preferences.\"\"\"\n",
        "        # Apply temperature and subtract max for numerical stability\n",
        "        scaled = preferences / self.temperature\n",
        "        exp_prefs = np.exp(scaled - np.max(scaled))\n",
        "        return exp_prefs / np.sum(exp_prefs)\n",
        "    \n",
        "    def get_action_probabilities(self, state: GameState) -> np.ndarray:\n",
        "        \"\"\"Get probability distribution over actions for given state.\"\"\"\n",
        "        state_key = state.current_sum\n",
        "        preferences = self.preferences[state_key]\n",
        "        return self.softmax(preferences)\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = True) -> int:\n",
        "        \"\"\"Select action using softmax over preferences.\"\"\"\n",
        "        probs = self.get_action_probabilities(state)\n",
        "        \n",
        "        if training:\n",
        "            # Sample from distribution\n",
        "            action_idx = np.random.choice(self.k, p=probs)\n",
        "        else:\n",
        "            # Greedy: select highest probability action\n",
        "            action_idx = np.argmax(probs)\n",
        "        \n",
        "        return action_idx + 1  # Actions are 1-indexed\n",
        "    \n",
        "    def update(self, state: GameState, action: int, reward: float):\n",
        "        \"\"\"Update preferences using gradient bandit update rule.\"\"\"\n",
        "        state_key = state.current_sum\n",
        "        action_idx = action - 1  # Convert to 0-indexed\n",
        "        \n",
        "        # Get current probabilities\n",
        "        probs = self.get_action_probabilities(state)\n",
        "        \n",
        "        # Update baseline (incremental mean)\n",
        "        self.state_visits[state_key] += 1\n",
        "        n = self.state_visits[state_key]\n",
        "        \n",
        "        if self.use_baseline:\n",
        "            old_baseline = self.reward_baseline[state_key]\n",
        "            self.reward_baseline[state_key] = old_baseline + (reward - old_baseline) / n\n",
        "            baseline = old_baseline  # Use old baseline for this update\n",
        "        else:\n",
        "            baseline = 0\n",
        "        \n",
        "        # Gradient bandit update\n",
        "        advantage = reward - baseline\n",
        "        \n",
        "        # Update all preferences\n",
        "        for a in range(self.k):\n",
        "            if a == action_idx:\n",
        "                # Selected action: positive gradient\n",
        "                self.preferences[state_key][a] += self.alpha * advantage * (1 - probs[a])\n",
        "            else:\n",
        "                # Non-selected actions: negative gradient\n",
        "                self.preferences[state_key][a] -= self.alpha * advantage * probs[a]\n",
        "    \n",
        "    def record_result(self, won: bool):\n",
        "        \"\"\"Record game result.\"\"\"\n",
        "        self.games += 1\n",
        "        if won:\n",
        "            self.wins += 1\n",
        "        else:\n",
        "            self.losses += 1\n",
        "    \n",
        "    def win_rate(self) -> float:\n",
        "        \"\"\"Calculate win rate.\"\"\"\n",
        "        return self.wins / max(1, self.games)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Loop\n",
        "\n",
        "We train both agents through self-play. Each agent updates its preferences based on the final game outcome, propagating the reward back through all states visited during the game.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gradient_bandit_agents(\n",
        "    game: AdditionGame,\n",
        "    agent1: GradientBanditAgent,\n",
        "    agent2: GradientBanditAgent,\n",
        "    num_episodes: int = 50000,\n",
        "    log_interval: int = 1000\n",
        ") -> dict:\n",
        "    \"\"\"Train two gradient bandit agents through self-play.\"\"\"\n",
        "    agents = [agent1, agent2]\n",
        "    stats = {\n",
        "        'episodes': [],\n",
        "        'agent1_win_rate': [],\n",
        "        'agent2_win_rate': [],\n",
        "        'avg_game_length': [],\n",
        "        'entropy_p1': [],\n",
        "        'entropy_p2': []\n",
        "    }\n",
        "    \n",
        "    game_lengths = []\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
        "        state = game.reset()\n",
        "        \n",
        "        # Store (state, action) pairs for each agent\n",
        "        history = [[], []]  # [agent1_history, agent2_history]\n",
        "        \n",
        "        while not game.done:\n",
        "            current_player = state.current_player\n",
        "            agent = agents[current_player]\n",
        "            \n",
        "            valid_actions = game.get_valid_actions()\n",
        "            action = agent.select_action(state, valid_actions, training=True)\n",
        "            \n",
        "            history[current_player].append((state, action))\n",
        "            \n",
        "            state, reward_p1, reward_p2, done = game.step(action)\n",
        "        \n",
        "        # Game ended - get final rewards\n",
        "        rewards = [reward_p1, reward_p2]\n",
        "        game_lengths.append(len(game.history))\n",
        "        \n",
        "        # Update each agent's preferences for all visited states\n",
        "        for player_id in [0, 1]:\n",
        "            agent = agents[player_id]\n",
        "            reward = rewards[player_id]\n",
        "            \n",
        "            # Update all state-action pairs visited by this agent\n",
        "            for state, action in history[player_id]:\n",
        "                agent.update(state, action, reward)\n",
        "            \n",
        "            agent.record_result(game.winner == player_id)\n",
        "        \n",
        "        # Log statistics\n",
        "        if (episode + 1) % log_interval == 0:\n",
        "            stats['episodes'].append(episode + 1)\n",
        "            stats['agent1_win_rate'].append(agent1.wins / max(1, agent1.games) * 100)\n",
        "            stats['agent2_win_rate'].append(agent2.wins / max(1, agent2.games) * 100)\n",
        "            stats['avg_game_length'].append(np.mean(game_lengths[-log_interval:]))\n",
        "            \n",
        "            # Calculate policy entropy (measure of exploration)\n",
        "            entropy1 = np.mean([\n",
        "                -np.sum(agent1.softmax(p) * np.log(agent1.softmax(p) + 1e-10))\n",
        "                for p in agent1.preferences.values() if len(p) > 0\n",
        "            ]) if agent1.preferences else 0\n",
        "            entropy2 = np.mean([\n",
        "                -np.sum(agent2.softmax(p) * np.log(agent2.softmax(p) + 1e-10))\n",
        "                for p in agent2.preferences.values() if len(p) > 0\n",
        "            ]) if agent2.preferences else 0\n",
        "            \n",
        "            stats['entropy_p1'].append(entropy1)\n",
        "            stats['entropy_p2'].append(entropy2)\n",
        "            \n",
        "            # Reset counters\n",
        "            agent1.wins = agent1.losses = agent1.games = 0\n",
        "            agent2.wins = agent2.losses = agent2.games = 0\n",
        "    \n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the Agents\n",
        "\n",
        "Let's train two gradient bandit agents on the Addition game with parameters `k=3` and `N=10`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Game parameters\n",
        "K = 3  # Can choose 1, 2, or 3\n",
        "N = 10  # Game ends when sum exceeds 10\n",
        "\n",
        "# Create game and agents\n",
        "game = AdditionGame(k=K, N=N)\n",
        "\n",
        "agent1 = GradientBanditAgent(\n",
        "    player_id=0,\n",
        "    k=K,\n",
        "    alpha=0.15,\n",
        "    use_baseline=True,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "agent2 = GradientBanditAgent(\n",
        "    player_id=1,\n",
        "    k=K,\n",
        "    alpha=0.15,\n",
        "    use_baseline=True,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(f\"Training Gradient Bandit agents on Addition game\")\n",
        "print(f\"Parameters: k={K}, N={N}\")\n",
        "print(f\"Player I starts, choices are integers from 1 to {K}\")\n",
        "print(f\"First player to make sum exceed {N} loses\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agents\n",
        "stats = train_gradient_bandit_agents(game, agent1, agent2, num_episodes=50000, log_interval=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Addition Game: Gradient Bandit Training Progress', fontsize=16, fontweight='bold', color='#e94560')\n",
        "\n",
        "# Win rates\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(stats['episodes'], stats['agent1_win_rate'], color='#f7b731', linewidth=2, label='Player I')\n",
        "ax1.plot(stats['episodes'], stats['agent2_win_rate'], color='#26de81', linewidth=2, label='Player II')\n",
        "ax1.axhline(y=50, color='#666', linestyle='--', alpha=0.5, label='50% baseline')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Win Rate (%)')\n",
        "ax1.set_title('Win Rates Over Training', color='#e94560')\n",
        "ax1.legend(facecolor='#16213e', edgecolor='#4a4a6a')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Policy entropy\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(stats['episodes'], stats['entropy_p1'], color='#f7b731', linewidth=2, label='Player I')\n",
        "ax2.plot(stats['episodes'], stats['entropy_p2'], color='#26de81', linewidth=2, label='Player II')\n",
        "max_entropy = np.log(K)\n",
        "ax2.axhline(y=max_entropy, color='#666', linestyle='--', alpha=0.5, label=f'Max entropy ({max_entropy:.2f})')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Policy Entropy')\n",
        "ax2.set_title('Policy Entropy (exploration measure)', color='#e94560')\n",
        "ax2.legend(facecolor='#16213e', edgecolor='#4a4a6a')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Average game length\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(stats['episodes'], stats['avg_game_length'], color='#a55eea', linewidth=2)\n",
        "ax3.set_xlabel('Episode')\n",
        "ax3.set_ylabel('Average Game Length (moves)')\n",
        "ax3.set_title('Average Game Length', color='#e94560')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Win rate difference\n",
        "ax4 = axes[1, 1]\n",
        "advantage = [a1 - a2 for a1, a2 in zip(stats['agent1_win_rate'], stats['agent2_win_rate'])]\n",
        "colors = ['#f7b731' if a > 0 else '#26de81' for a in advantage]\n",
        "ax4.bar(stats['episodes'], advantage, color=colors, alpha=0.7, width=400)\n",
        "ax4.axhline(y=0, color='#fff', linewidth=1)\n",
        "ax4.set_xlabel('Episode')\n",
        "ax4.set_ylabel('Win Rate Difference (%)')\n",
        "ax4.set_title('Player I Advantage (positive = Player I favored)', color='#e94560')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyzing Learned Policies\n",
        "\n",
        "Let's examine the action probabilities learned by each agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_gradient_bandit_policy(agent: GradientBanditAgent, k: int, N: int, player_name: str):\n",
        "    \"\"\"Visualize the learned policy as probability distributions.\"\"\"\n",
        "    \n",
        "    sums = list(range(0, N + 1))\n",
        "    actions = list(range(1, k + 1))\n",
        "    \n",
        "    # Create probability and preference matrices\n",
        "    prob_matrix = np.zeros((len(sums), len(actions)))\n",
        "    pref_matrix = np.zeros((len(sums), len(actions)))\n",
        "    \n",
        "    for i, s in enumerate(sums):\n",
        "        prefs = agent.preferences[s]\n",
        "        probs = agent.softmax(prefs)\n",
        "        for j in range(k):\n",
        "            prob_matrix[i, j] = probs[j]\n",
        "            pref_matrix[i, j] = prefs[j]\n",
        "    \n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n",
        "    fig.suptitle(f'{player_name} Learned Strategy (Gradient Bandit)', fontsize=14, fontweight='bold', color='#e94560')\n",
        "    \n",
        "    # Preferences heatmap\n",
        "    im1 = ax1.imshow(pref_matrix, aspect='auto', cmap='RdYlBu_r')\n",
        "    ax1.set_yticks(range(len(sums)))\n",
        "    ax1.set_yticklabels(sums)\n",
        "    ax1.set_xticks(range(len(actions)))\n",
        "    ax1.set_xticklabels(actions)\n",
        "    ax1.set_ylabel('Current Sum')\n",
        "    ax1.set_xlabel('Action')\n",
        "    ax1.set_title('Preferences H(s,a)', color='#e94560')\n",
        "    plt.colorbar(im1, ax=ax1, label='Preference')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(len(actions)):\n",
        "            color = 'white' if abs(pref_matrix[i, j]) > 0.5 else 'black'\n",
        "            ax1.text(j, i, f'{pref_matrix[i, j]:.2f}', ha='center', va='center', color=color, fontsize=8)\n",
        "    \n",
        "    # Probabilities heatmap\n",
        "    im2 = ax2.imshow(prob_matrix, aspect='auto', cmap='Greens', vmin=0, vmax=1)\n",
        "    ax2.set_yticks(range(len(sums)))\n",
        "    ax2.set_yticklabels(sums)\n",
        "    ax2.set_xticks(range(len(actions)))\n",
        "    ax2.set_xticklabels(actions)\n",
        "    ax2.set_ylabel('Current Sum')\n",
        "    ax2.set_xlabel('Action')\n",
        "    ax2.set_title('Action Probabilities π(a|s)', color='#e94560')\n",
        "    plt.colorbar(im2, ax=ax2, label='Probability')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(len(actions)):\n",
        "            color = 'white' if prob_matrix[i, j] > 0.5 else 'black'\n",
        "            ax2.text(j, i, f'{prob_matrix[i, j]:.2f}', ha='center', va='center', color=color, fontsize=8)\n",
        "    \n",
        "    # Best action visualization\n",
        "    best_actions = np.argmax(prob_matrix, axis=1) + 1\n",
        "    colors = ['#f7b731', '#26de81', '#a55eea']\n",
        "    \n",
        "    ax3.barh(sums, best_actions, color=[colors[a-1] for a in best_actions], edgecolor='white', linewidth=0.5)\n",
        "    ax3.set_xlabel('Most Probable Action')\n",
        "    ax3.set_ylabel('Current Sum')\n",
        "    ax3.set_title('Greedy Policy', color='#e94560')\n",
        "    ax3.set_xticks([1, 2, 3])\n",
        "    ax3.set_xlim(0, k + 0.5)\n",
        "    ax3.invert_yaxis()\n",
        "    \n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=colors[i], label=f'Action {i+1}') for i in range(k)]\n",
        "    ax3.legend(handles=legend_elements, loc='lower right', facecolor='#16213e', edgecolor='#4a4a6a')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return best_actions, prob_matrix\n",
        "\n",
        "print(\"Player I Strategy:\")\n",
        "p1_actions, p1_probs = visualize_gradient_bandit_policy(agent1, K, N, \"Player I\")\n",
        "\n",
        "print(\"\\nPlayer II Strategy:\")\n",
        "p2_actions, p2_probs = visualize_gradient_bandit_policy(agent2, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Theoretical Optimal Strategy\n",
        "\n",
        "For the Addition game, the optimal strategy is deterministic and based on modular arithmetic:\n",
        "\n",
        "- **Losing positions**: Sums where `sum ≡ N (mod k+1)`\n",
        "- **Winning positions**: All other sums\n",
        "\n",
        "From a winning position, play the action that moves the opponent to a losing position.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_with_optimal(agent: GradientBanditAgent, optimal_actions: dict, losing_positions: list, \n",
        "                         k: int, N: int, player_name: str):\n",
        "    \"\"\"Compare learned policy with optimal.\"\"\"\n",
        "    matches = 0\n",
        "    mismatches = []\n",
        "    \n",
        "    for s in range(0, N + 1):\n",
        "        # Get learned action (greedy)\n",
        "        probs = agent.get_action_probabilities(GameState(s, 0))\n",
        "        learned_action = np.argmax(probs) + 1\n",
        "        optimal = optimal_actions[s]\n",
        "        \n",
        "        if learned_action == optimal:\n",
        "            matches += 1\n",
        "        else:\n",
        "            # Check if learned action is also winning\n",
        "            next_sum_learned = s + learned_action\n",
        "            next_sum_optimal = s + optimal\n",
        "            both_win = (next_sum_learned in losing_positions) and (next_sum_optimal in losing_positions)\n",
        "            \n",
        "            if both_win:\n",
        "                matches += 1\n",
        "            else:\n",
        "                mismatches.append((s, learned_action, optimal, probs))\n",
        "    \n",
        "    accuracy = matches / (N + 1) * 100\n",
        "    \n",
        "    print(f\"\\n{player_name} Policy Comparison:\")\n",
        "    print(f\"  Accuracy: {accuracy:.1f}% ({matches}/{N + 1} positions)\")\n",
        "    \n",
        "    if mismatches:\n",
        "        print(f\"  Mismatches:\")\n",
        "        for s, learned, optimal, probs in mismatches[:5]:\n",
        "            probs_str = \", \".join([f\"{p:.2f}\" for p in probs])\n",
        "            print(f\"    Sum {s}: learned={learned}, optimal={optimal}, probs=[{probs_str}]\")\n",
        "    else:\n",
        "        print(\"  Perfect match with optimal strategy!\")\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "acc1 = compare_with_optimal(agent1, optimal_actions, losing_positions, K, N, \"Player I\")\n",
        "acc2 = compare_with_optimal(agent2, optimal_actions, losing_positions, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Against Optimal Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgent:\n",
        "    \"\"\"Agent that plays the theoretically optimal strategy.\"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        _, self.optimal_actions = compute_optimal_strategy(k, N)\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = False) -> int:\n",
        "        return self.optimal_actions.get(state.current_sum, 1)\n",
        "\n",
        "\n",
        "def evaluate(game: AdditionGame, agent1, agent2, num_games: int = 1000):\n",
        "    \"\"\"Evaluate two agents over multiple games.\"\"\"\n",
        "    wins = [0, 0]\n",
        "    \n",
        "    for _ in range(num_games):\n",
        "        state = game.reset()\n",
        "        agents = [agent1, agent2]\n",
        "        \n",
        "        while not game.done:\n",
        "            agent = agents[state.current_player]\n",
        "            action = agent.select_action(state, game.get_valid_actions(), training=False)\n",
        "            state, _, _, _ = game.step(action)\n",
        "        \n",
        "        wins[game.winner] += 1\n",
        "    \n",
        "    return wins[0], wins[1]\n",
        "\n",
        "\n",
        "optimal_agent = OptimalAgent(K, N)\n",
        "\n",
        "print(\"Evaluation Results (1000 games each):\\n\")\n",
        "\n",
        "print(\"Test 1: Learned Player I vs Optimal Player II\")\n",
        "w1, w2 = evaluate(game, agent1, optimal_agent, num_games=1000)\n",
        "print(f\"  Learned wins: {w1}, Optimal wins: {w2}\")\n",
        "print(f\"  Learned win rate: {w1/10:.1f}%\")\n",
        "\n",
        "print(\"\\nTest 2: Optimal Player I vs Learned Player II\")\n",
        "w1, w2 = evaluate(game, optimal_agent, agent2, num_games=1000)\n",
        "print(f\"  Optimal wins: {w1}, Learned wins: {w2}\")\n",
        "print(f\"  Learned win rate: {w2/10:.1f}%\")\n",
        "\n",
        "print(\"\\nTest 3: Learned vs Learned (self-play)\")\n",
        "w1, w2 = evaluate(game, agent1, agent2, num_games=1000)\n",
        "print(f\"  Player I wins: {w1}, Player II wins: {w2}\")\n",
        "print(f\"  Player I win rate: {w1/10:.1f}%\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Note: With k=3, N=10, optimal Player I should always win.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Watching a Sample Game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_game_verbose(game: AdditionGame, agent1, agent2, name1: str, name2: str, show_probs: bool = True):\n",
        "    \"\"\"Play a game with detailed output.\"\"\"\n",
        "    state = game.reset()\n",
        "    agents = [agent1, agent2]\n",
        "    names = [name1, name2]\n",
        "    \n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ADDITION GAME: k={game.k}, N={game.N}\")\n",
        "    print(f\"{name1} (Player I) vs {name2} (Player II)\")\n",
        "    print(f\"First to make sum exceed {game.N} loses!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    move = 1\n",
        "    while not game.done:\n",
        "        player = state.current_player\n",
        "        agent = agents[player]\n",
        "        name = names[player]\n",
        "        \n",
        "        action = agent.select_action(state, game.get_valid_actions(), training=False)\n",
        "        \n",
        "        # Show probabilities if agent is gradient bandit\n",
        "        if show_probs and hasattr(agent, 'get_action_probabilities'):\n",
        "            probs = agent.get_action_probabilities(state)\n",
        "            probs_str = \", \".join([f\"{p:.2f}\" for p in probs])\n",
        "            print(f\"Move {move}: {name} (probs: [{probs_str}]) -> chooses {action}\")\n",
        "        else:\n",
        "            print(f\"Move {move}: {name} chooses {action}\")\n",
        "        \n",
        "        print(f\"         Sum: {state.current_sum} -> {state.current_sum + action}\")\n",
        "        \n",
        "        state, _, _, done = game.step(action)\n",
        "        \n",
        "        if done:\n",
        "            print(f\"\\n         *** SUM ({state.current_sum}) EXCEEDS {game.N}! ***\")\n",
        "            print(f\"\\n  WINNER: {names[game.winner]}\")\n",
        "        else:\n",
        "            print()\n",
        "        \n",
        "        move += 1\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"History: {game.history}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "print(\"Game 1: Learned agents playing each other\\n\")\n",
        "play_game_verbose(game, agent1, agent2, \"Learned-I\", \"Learned-II\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Game 2: Learned Player I vs Optimal Player II\\n\")\n",
        "play_game_verbose(game, agent1, optimal_agent, \"Learned-I\", \"Optimal-II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "We implemented **Gradient Bandit** agents for Blackwell's Addition game. Key aspects:\n",
        "\n",
        "1. **Preference-Based Learning**: Instead of estimating action values (like Q-learning), gradient bandits learn action *preferences* and use softmax to convert them to probabilities.\n",
        "\n",
        "2. **Stochastic Policies**: The softmax distribution naturally provides exploration through stochastic action selection, avoiding the need for epsilon-greedy.\n",
        "\n",
        "3. **Baseline Importance**: Using a baseline (average reward) significantly improves learning stability by reducing variance in the gradient estimates.\n",
        "\n",
        "### Gradient Bandit vs Q-Learning for This Game\n",
        "\n",
        "| Aspect | Gradient Bandit | Q-Learning |\n",
        "|--------|----------------|------------|\n",
        "| Policy Type | Stochastic (softmax) | Deterministic (argmax) |\n",
        "| Exploration | Built-in via softmax | Requires epsilon-greedy |\n",
        "| What it learns | Action preferences | Action values |\n",
        "| Update rule | Policy gradient | Bellman equation |\n",
        "\n",
        "### Connections to Blackwell's Work\n",
        "\n",
        "While the Addition game has a deterministic optimal solution (making Q-learning naturally suited), the gradient bandit approach demonstrates how policy gradient methods can also discover optimal play through iterative preference updates. This connects to broader themes in Blackwell's work on:\n",
        "- Sequential decision problems\n",
        "- Adaptive strategies that converge to optimality\n",
        "- The interplay between exploration and exploitation\n",
        "\n",
        "### Extensions\n",
        "\n",
        "- **Actor-Critic**: Combine gradient bandits with value estimation\n",
        "- **Natural Gradient**: Use Fisher information for more efficient updates\n",
        "- **Larger Games**: Deep policy gradients for games with larger state spaces\n",
        "- **Imperfect Information**: Extend to games with hidden information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_optimal_strategy(k: int, N: int):\n",
        "    \"\"\"Compute the theoretically optimal strategy.\"\"\"\n",
        "    # Losing positions: sums where sum ≡ N (mod k+1)\n",
        "    losing_positions = []\n",
        "    s = N\n",
        "    while s >= 0:\n",
        "        losing_positions.append(s)\n",
        "        s -= (k + 1)\n",
        "    losing_positions = sorted(losing_positions)\n",
        "    \n",
        "    # Optimal actions from each position\n",
        "    optimal_actions = {}\n",
        "    for s in range(0, N + 1):\n",
        "        if s in losing_positions:\n",
        "            optimal_actions[s] = 1  # Any move loses, pick smallest\n",
        "        else:\n",
        "            for a in range(1, k + 1):\n",
        "                next_sum = s + a\n",
        "                if next_sum in losing_positions:\n",
        "                    optimal_actions[s] = a\n",
        "                    break\n",
        "            else:\n",
        "                optimal_actions[s] = 1\n",
        "    \n",
        "    return losing_positions, optimal_actions\n",
        "\n",
        "losing_positions, optimal_actions = compute_optimal_strategy(K, N)\n",
        "\n",
        "print(f\"Game parameters: k={K}, N={N}\")\n",
        "print(f\"\\nLosing positions (for player to move): {losing_positions}\")\n",
        "print(f\"These satisfy: sum ≡ {N} (mod {K+1})\")\n",
        "\n",
        "print(f\"\\nOptimal actions from each position:\")\n",
        "for s in range(0, N + 1):\n",
        "    status = \"LOSING\" if s in losing_positions else \"winning\"\n",
        "    print(f\"  Sum {s:2d}: play {optimal_actions[s]} ({status})\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "if 0 in losing_positions:\n",
        "    print(\"Initial position (0) is LOSING → Player II wins with optimal play!\")\n",
        "else:\n",
        "    print(\"Initial position (0) is winning → Player I wins with optimal play!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
