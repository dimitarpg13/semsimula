{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addition Game: Monte Carlo Agents\n",
        "\n",
        "## Game Definition (from Blackwell's \"Game Theory and Statistical Decisions\")\n",
        "\n",
        "> *The parameters of the game k and N are given. Player I and Player II alternately choose integers, each choice being one of the integers 1,...,k and each choice made with the knowledge of all preceding choices. As soon as the sum of the chosen integers exceeds N, the last player to choose loses the game and pays his opponent one unit.*\n",
        "\n",
        "In this notebook, we implement **Monte Carlo** reinforcement learning agents. Unlike Q-learning which bootstraps (updates based on estimated future values), Monte Carlo methods learn directly from complete episodes by averaging actual returns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['font.family'] = 'monospace'\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['axes.facecolor'] = '#1a1a2e'\n",
        "plt.rcParams['figure.facecolor'] = '#0f0f1a'\n",
        "plt.rcParams['axes.edgecolor'] = '#533483'\n",
        "plt.rcParams['axes.labelcolor'] = '#ff6f3c'\n",
        "plt.rcParams['xtick.color'] = '#a0a0a0'\n",
        "plt.rcParams['ytick.color'] = '#a0a0a0'\n",
        "plt.rcParams['text.color'] = '#e0e0e0'\n",
        "plt.rcParams['grid.color'] = '#2d2d44'\n",
        "plt.rcParams['grid.alpha'] = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Game Environment\n",
        "\n",
        "The state of the game is characterized by:\n",
        "- The current sum of all chosen integers\n",
        "- Whose turn it is (Player I or Player II)\n",
        "\n",
        "The game ends when the sum exceeds N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GameState:\n",
        "    \"\"\"Represents the current state of the Addition game.\"\"\"\n",
        "    current_sum: int\n",
        "    current_player: int  # 0 for Player I, 1 for Player II\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash((self.current_sum, self.current_player))\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return (self.current_sum == other.current_sum and \n",
        "                self.current_player == other.current_player)\n",
        "\n",
        "\n",
        "class AdditionGame:\n",
        "    \"\"\"The Addition game environment.\n",
        "    \n",
        "    Parameters:\n",
        "        k: Maximum integer that can be chosen (choices are 1, 2, ..., k)\n",
        "        N: Threshold - the game ends when sum exceeds N\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self) -> GameState:\n",
        "        \"\"\"Reset the game to initial state.\"\"\"\n",
        "        self.state = GameState(current_sum=0, current_player=0)\n",
        "        self.history: List[int] = []\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.state\n",
        "    \n",
        "    def get_valid_actions(self) -> List[int]:\n",
        "        \"\"\"Returns list of valid actions (1 to k).\"\"\"\n",
        "        return list(range(1, self.k + 1))\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[GameState, float, float, bool]:\n",
        "        \"\"\"Execute an action and return (new_state, reward_p1, reward_p2, done).\"\"\"\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game is already over!\")\n",
        "        \n",
        "        if action < 1 or action > self.k:\n",
        "            raise ValueError(f\"Invalid action {action}. Must be in [1, {self.k}]\")\n",
        "        \n",
        "        self.history.append(action)\n",
        "        current_player = self.state.current_player\n",
        "        new_sum = self.state.current_sum + action\n",
        "        \n",
        "        if new_sum > self.N:\n",
        "            self.done = True\n",
        "            self.winner = 1 - current_player\n",
        "            reward_p1 = 1.0 if self.winner == 0 else -1.0\n",
        "            reward_p2 = -reward_p1\n",
        "        else:\n",
        "            reward_p1 = 0.0\n",
        "            reward_p2 = 0.0\n",
        "        \n",
        "        self.state = GameState(\n",
        "            current_sum=new_sum,\n",
        "            current_player=1 - current_player\n",
        "        )\n",
        "        \n",
        "        return self.state, reward_p1, reward_p2, self.done\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Print the current game state.\"\"\"\n",
        "        print(f\"Sum: {self.state.current_sum} / N={self.N}\")\n",
        "        print(f\"History: {self.history}\")\n",
        "        print(f\"Current player: {'I' if self.state.current_player == 0 else 'II'}\")\n",
        "        if self.done:\n",
        "            print(f\"Winner: Player {'I' if self.winner == 0 else 'II'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Monte Carlo Methods\n",
        "\n",
        "**Monte Carlo** methods learn value functions from complete episodes of experience. Key characteristics:\n",
        "\n",
        "1. **No bootstrapping**: Unlike TD methods (Q-learning), MC methods wait until the end of an episode to update values\n",
        "2. **Sample returns**: Q(s,a) is estimated by averaging returns observed after taking action a in state s\n",
        "3. **First-visit vs Every-visit**: \n",
        "   - First-visit MC: Only count the first time (s,a) is visited in an episode\n",
        "   - Every-visit MC: Count every time (s,a) is visited\n",
        "\n",
        "For the Addition game, since states can only be visited once per episode (sum always increases), first-visit and every-visit are equivalent.\n",
        "\n",
        "### Update Rule (Incremental Mean)\n",
        "\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\frac{1}{N(s,a)}(G - Q(s,a))$$\n",
        "\n",
        "where $G$ is the return (final reward) and $N(s,a)$ is the visit count.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MonteCarloAgent:\n",
        "    \"\"\"Monte Carlo agent for the Addition game.\n",
        "    \n",
        "    Uses first-visit MC with epsilon-greedy exploration.\n",
        "    Learns Q(s,a) by averaging returns from complete episodes.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        player_id: int,\n",
        "        k: int,\n",
        "        epsilon_start: float = 1.0,\n",
        "        epsilon_end: float = 0.05,\n",
        "        epsilon_decay: float = 0.9995,\n",
        "        use_constant_alpha: bool = False,\n",
        "        alpha: float = 0.1\n",
        "    ):\n",
        "        self.player_id = player_id\n",
        "        self.k = k\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.use_constant_alpha = use_constant_alpha\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # Q-values: Q[state_key][action] = value\n",
        "        self.Q: Dict[int, Dict[int, float]] = defaultdict(lambda: {a: 0.0 for a in range(1, k + 1)})\n",
        "        \n",
        "        # Visit counts for incremental mean update\n",
        "        self.N: Dict[int, Dict[int, int]] = defaultdict(lambda: {a: 0 for a in range(1, k + 1)})\n",
        "        \n",
        "        # Statistics\n",
        "        self.wins = 0\n",
        "        self.losses = 0\n",
        "        self.games = 0\n",
        "    \n",
        "    def get_state_key(self, state: GameState) -> int:\n",
        "        \"\"\"Convert state to key for Q-table lookup.\"\"\"\n",
        "        return state.current_sum\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = True) -> int:\n",
        "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "        \n",
        "        state_key = self.get_state_key(state)\n",
        "        q_values = self.Q[state_key]\n",
        "        \n",
        "        # Find action with highest Q-value\n",
        "        best_action = max(valid_actions, key=lambda a: q_values[a])\n",
        "        \n",
        "        # If all Q-values are 0 (unvisited), choose randomly\n",
        "        if all(q_values[a] == 0 for a in valid_actions):\n",
        "            return random.choice(valid_actions)\n",
        "        \n",
        "        return best_action\n",
        "    \n",
        "    def update_from_episode(self, episode: List[Tuple[int, int]], final_reward: float):\n",
        "        \"\"\"Update Q-values from a complete episode.\n",
        "        \n",
        "        Args:\n",
        "            episode: List of (state_key, action) tuples visited by this agent\n",
        "            final_reward: The return (reward) received at the end of the episode\n",
        "        \"\"\"\n",
        "        # In this game, the return is the same for all state-action pairs\n",
        "        # (no intermediate rewards, only final +1 or -1)\n",
        "        G = final_reward\n",
        "        \n",
        "        # First-visit MC: only update first occurrence of each (s, a)\n",
        "        visited = set()\n",
        "        \n",
        "        for state_key, action in episode:\n",
        "            if (state_key, action) not in visited:\n",
        "                visited.add((state_key, action))\n",
        "                \n",
        "                # Increment visit count\n",
        "                self.N[state_key][action] += 1\n",
        "                n = self.N[state_key][action]\n",
        "                \n",
        "                # Update Q-value using incremental mean or constant alpha\n",
        "                if self.use_constant_alpha:\n",
        "                    # Constant step-size (better for non-stationary)\n",
        "                    self.Q[state_key][action] += self.alpha * (G - self.Q[state_key][action])\n",
        "                else:\n",
        "                    # Sample average (converges to true mean)\n",
        "                    self.Q[state_key][action] += (G - self.Q[state_key][action]) / n\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    def record_result(self, won: bool):\n",
        "        \"\"\"Record game result.\"\"\"\n",
        "        self.games += 1\n",
        "        if won:\n",
        "            self.wins += 1\n",
        "        else:\n",
        "            self.losses += 1\n",
        "    \n",
        "    def win_rate(self) -> float:\n",
        "        \"\"\"Calculate win rate.\"\"\"\n",
        "        return self.wins / max(1, self.games)\n",
        "    \n",
        "    def get_policy(self) -> Dict[int, int]:\n",
        "        \"\"\"Return greedy policy.\"\"\"\n",
        "        policy = {}\n",
        "        for state_key in self.Q:\n",
        "            q_values = self.Q[state_key]\n",
        "            policy[state_key] = max(range(1, self.k + 1), key=lambda a: q_values[a])\n",
        "        return policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Loop\n",
        "\n",
        "We train both agents through self-play. After each complete game (episode), both agents update their Q-values based on the observed return.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the Agents\n",
        "\n",
        "Let's train two Monte Carlo agents on the Addition game with parameters `k=3` and `N=10`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Game parameters\n",
        "K = 3  # Can choose 1, 2, or 3\n",
        "N = 10  # Game ends when sum exceeds 10\n",
        "\n",
        "# Create game and agents\n",
        "game = AdditionGame(k=K, N=N)\n",
        "\n",
        "agent1 = MonteCarloAgent(\n",
        "    player_id=0,\n",
        "    k=K,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay=0.9997,\n",
        "    use_constant_alpha=False  # Use sample average\n",
        ")\n",
        "\n",
        "agent2 = MonteCarloAgent(\n",
        "    player_id=1,\n",
        "    k=K,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay=0.9997,\n",
        "    use_constant_alpha=False\n",
        ")\n",
        "\n",
        "print(f\"Training Monte Carlo agents on Addition game\")\n",
        "print(f\"Parameters: k={K}, N={N}\")\n",
        "print(f\"Player I starts, choices are integers from 1 to {K}\")\n",
        "print(f\"First player to make sum exceed {N} loses\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agents\n",
        "stats = train_monte_carlo_agents(game, agent1, agent2, num_episodes=50000, log_interval=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Addition Game: Monte Carlo Training Progress', fontsize=16, fontweight='bold', color='#ff6f3c')\n",
        "\n",
        "# Win rates\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(stats['episodes'], stats['agent1_win_rate'], color='#ffc13b', linewidth=2, label='Player I')\n",
        "ax1.plot(stats['episodes'], stats['agent2_win_rate'], color='#1eb980', linewidth=2, label='Player II')\n",
        "ax1.axhline(y=50, color='#666', linestyle='--', alpha=0.5, label='50% baseline')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Win Rate (%)')\n",
        "ax1.set_title('Win Rates Over Training', color='#ff6f3c')\n",
        "ax1.legend(facecolor='#1a1a2e', edgecolor='#533483')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Epsilon decay\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(stats['episodes'], stats['epsilon'], color='#ff6f3c', linewidth=2)\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Epsilon')\n",
        "ax2.set_title('Exploration Rate Decay', color='#ff6f3c')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Average game length\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(stats['episodes'], stats['avg_game_length'], color='#a29bfe', linewidth=2)\n",
        "ax3.set_xlabel('Episode')\n",
        "ax3.set_ylabel('Average Game Length (moves)')\n",
        "ax3.set_title('Average Game Length', color='#ff6f3c')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Total visits (learning progress indicator)\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(stats['episodes'], stats['total_visits'], color='#fd79a8', linewidth=2)\n",
        "ax4.set_xlabel('Episode')\n",
        "ax4.set_ylabel('Total State-Action Visits')\n",
        "ax4.set_title('Cumulative Experience (Player I)', color='#ff6f3c')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyzing Learned Q-Values and Policies\n",
        "\n",
        "Let's examine the Q-values and visit counts learned by each agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_mc_policy(agent: MonteCarloAgent, k: int, N: int, player_name: str):\n",
        "    \"\"\"Visualize the learned Q-values and visit counts.\"\"\"\n",
        "    \n",
        "    sums = list(range(0, N + 1))\n",
        "    actions = list(range(1, k + 1))\n",
        "    \n",
        "    # Create Q-value and visit count matrices\n",
        "    q_matrix = np.zeros((len(sums), len(actions)))\n",
        "    visit_matrix = np.zeros((len(sums), len(actions)))\n",
        "    \n",
        "    for i, s in enumerate(sums):\n",
        "        for j, a in enumerate(actions):\n",
        "            q_matrix[i, j] = agent.Q[s][a]\n",
        "            visit_matrix[i, j] = agent.N[s][a]\n",
        "    \n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n",
        "    fig.suptitle(f'{player_name} Learned Strategy (Monte Carlo)', fontsize=14, fontweight='bold', color='#ff6f3c')\n",
        "    \n",
        "    # Q-value heatmap\n",
        "    im1 = ax1.imshow(q_matrix, aspect='auto', cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "    ax1.set_yticks(range(len(sums)))\n",
        "    ax1.set_yticklabels(sums)\n",
        "    ax1.set_xticks(range(len(actions)))\n",
        "    ax1.set_xticklabels(actions)\n",
        "    ax1.set_ylabel('Current Sum')\n",
        "    ax1.set_xlabel('Action')\n",
        "    ax1.set_title('Q-Values', color='#ff6f3c')\n",
        "    plt.colorbar(im1, ax=ax1, label='Q-value')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(len(actions)):\n",
        "            color = 'white' if abs(q_matrix[i, j]) > 0.5 else 'black'\n",
        "            ax1.text(j, i, f'{q_matrix[i, j]:.2f}', ha='center', va='center', color=color, fontsize=8)\n",
        "    \n",
        "    # Visit count heatmap\n",
        "    im2 = ax2.imshow(visit_matrix, aspect='auto', cmap='Blues')\n",
        "    ax2.set_yticks(range(len(sums)))\n",
        "    ax2.set_yticklabels(sums)\n",
        "    ax2.set_xticks(range(len(actions)))\n",
        "    ax2.set_xticklabels(actions)\n",
        "    ax2.set_ylabel('Current Sum')\n",
        "    ax2.set_xlabel('Action')\n",
        "    ax2.set_title('Visit Counts N(s,a)', color='#ff6f3c')\n",
        "    plt.colorbar(im2, ax=ax2, label='Visits')\n",
        "    \n",
        "    for i in range(len(sums)):\n",
        "        for j in range(len(actions)):\n",
        "            color = 'white' if visit_matrix[i, j] > visit_matrix.max() * 0.5 else 'black'\n",
        "            ax2.text(j, i, f'{int(visit_matrix[i, j])}', ha='center', va='center', color=color, fontsize=8)\n",
        "    \n",
        "    # Best action visualization\n",
        "    best_actions = np.argmax(q_matrix, axis=1) + 1\n",
        "    colors = ['#ffc13b', '#1eb980', '#a29bfe']\n",
        "    \n",
        "    ax3.barh(sums, best_actions, color=[colors[a-1] for a in best_actions], edgecolor='white', linewidth=0.5)\n",
        "    ax3.set_xlabel('Greedy Action')\n",
        "    ax3.set_ylabel('Current Sum')\n",
        "    ax3.set_title('Learned Greedy Policy', color='#ff6f3c')\n",
        "    ax3.set_xticks([1, 2, 3])\n",
        "    ax3.set_xlim(0, k + 0.5)\n",
        "    ax3.invert_yaxis()\n",
        "    \n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=colors[i], label=f'Action {i+1}') for i in range(k)]\n",
        "    ax3.legend(handles=legend_elements, loc='lower right', facecolor='#1a1a2e', edgecolor='#533483')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return best_actions\n",
        "\n",
        "print(\"Player I Strategy:\")\n",
        "p1_actions = visualize_mc_policy(agent1, K, N, \"Player I\")\n",
        "\n",
        "print(\"\\nPlayer II Strategy:\")\n",
        "p2_actions = visualize_mc_policy(agent2, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Theoretical Optimal Strategy\n",
        "\n",
        "The Addition game has a known optimal strategy based on modular arithmetic:\n",
        "\n",
        "- **Losing positions** (for the player to move): Sums satisfying `sum ≡ N (mod k+1)`\n",
        "- **Winning positions**: All other sums\n",
        "\n",
        "The optimal play is to move the opponent to a losing position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_optimal_strategy(k: int, N: int):\n",
        "    \"\"\"Compute the theoretically optimal strategy.\"\"\"\n",
        "    # Losing positions: sums where sum ≡ N (mod k+1)\n",
        "    losing_positions = []\n",
        "    s = N\n",
        "    while s >= 0:\n",
        "        losing_positions.append(s)\n",
        "        s -= (k + 1)\n",
        "    losing_positions = sorted(losing_positions)\n",
        "    \n",
        "    # Optimal actions\n",
        "    optimal_actions = {}\n",
        "    for s in range(0, N + 1):\n",
        "        if s in losing_positions:\n",
        "            optimal_actions[s] = 1  # Losing position, any move loses\n",
        "        else:\n",
        "            for a in range(1, k + 1):\n",
        "                if s + a in losing_positions:\n",
        "                    optimal_actions[s] = a\n",
        "                    break\n",
        "            else:\n",
        "                optimal_actions[s] = 1\n",
        "    \n",
        "    return losing_positions, optimal_actions\n",
        "\n",
        "losing_positions, optimal_actions = compute_optimal_strategy(K, N)\n",
        "\n",
        "print(f\"Game parameters: k={K}, N={N}\")\n",
        "print(f\"\\nLosing positions (for player to move): {losing_positions}\")\n",
        "print(f\"These satisfy: sum ≡ {N} (mod {K+1})\")\n",
        "\n",
        "print(f\"\\nOptimal actions from each position:\")\n",
        "for s in range(0, N + 1):\n",
        "    status = \"LOSING\" if s in losing_positions else \"winning\"\n",
        "    print(f\"  Sum {s:2d}: play {optimal_actions[s]} ({status})\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "if 0 in losing_positions:\n",
        "    print(\"Initial position (0) is LOSING -> Player II wins with optimal play!\")\n",
        "else:\n",
        "    print(\"Initial position (0) is winning -> Player I wins with optimal play!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_with_optimal(agent: MonteCarloAgent, optimal_actions: dict, losing_positions: list,\n",
        "                         k: int, N: int, player_name: str):\n",
        "    \"\"\"Compare learned policy with optimal.\"\"\"\n",
        "    matches = 0\n",
        "    mismatches = []\n",
        "    \n",
        "    for s in range(0, N + 1):\n",
        "        # Get learned action (greedy)\n",
        "        q_values = agent.Q[s]\n",
        "        learned_action = max(range(1, k + 1), key=lambda a: q_values[a])\n",
        "        optimal = optimal_actions[s]\n",
        "        \n",
        "        if learned_action == optimal:\n",
        "            matches += 1\n",
        "        else:\n",
        "            # Check if learned action is also winning\n",
        "            next_sum_learned = s + learned_action\n",
        "            next_sum_optimal = s + optimal\n",
        "            both_win = (next_sum_learned in losing_positions) and (next_sum_optimal in losing_positions)\n",
        "            \n",
        "            if both_win:\n",
        "                matches += 1\n",
        "            else:\n",
        "                mismatches.append((s, learned_action, optimal, q_values))\n",
        "    \n",
        "    accuracy = matches / (N + 1) * 100\n",
        "    \n",
        "    print(f\"\\n{player_name} Policy Comparison:\")\n",
        "    print(f\"  Accuracy: {accuracy:.1f}% ({matches}/{N + 1} positions)\")\n",
        "    \n",
        "    if mismatches:\n",
        "        print(f\"  Mismatches:\")\n",
        "        for s, learned, optimal, q_vals in mismatches[:5]:\n",
        "            q_str = \", \".join([f\"Q({a})={q_vals[a]:.2f}\" for a in range(1, k + 1)])\n",
        "            print(f\"    Sum {s}: learned={learned}, optimal={optimal} [{q_str}]\")\n",
        "    else:\n",
        "        print(\"  Perfect match with optimal strategy!\")\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "acc1 = compare_with_optimal(agent1, optimal_actions, losing_positions, K, N, \"Player I\")\n",
        "acc2 = compare_with_optimal(agent2, optimal_actions, losing_positions, K, N, \"Player II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Against Optimal Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgent:\n",
        "    \"\"\"Agent that plays the theoretically optimal strategy.\"\"\"\n",
        "    \n",
        "    def __init__(self, k: int, N: int):\n",
        "        _, self.optimal_actions = compute_optimal_strategy(k, N)\n",
        "    \n",
        "    def select_action(self, state: GameState, valid_actions: List[int], training: bool = False) -> int:\n",
        "        return self.optimal_actions.get(state.current_sum, 1)\n",
        "\n",
        "\n",
        "def evaluate(game: AdditionGame, agent1, agent2, num_games: int = 1000):\n",
        "    \"\"\"Evaluate two agents over multiple games.\"\"\"\n",
        "    wins = [0, 0]\n",
        "    \n",
        "    for _ in range(num_games):\n",
        "        state = game.reset()\n",
        "        agents = [agent1, agent2]\n",
        "        \n",
        "        while not game.done:\n",
        "            agent = agents[state.current_player]\n",
        "            action = agent.select_action(state, game.get_valid_actions(), training=False)\n",
        "            state, _, _, _ = game.step(action)\n",
        "        \n",
        "        wins[game.winner] += 1\n",
        "    \n",
        "    return wins[0], wins[1]\n",
        "\n",
        "\n",
        "optimal_agent = OptimalAgent(K, N)\n",
        "\n",
        "print(\"Evaluation Results (1000 games each):\\n\")\n",
        "\n",
        "print(\"Test 1: MC Player I vs Optimal Player II\")\n",
        "w1, w2 = evaluate(game, agent1, optimal_agent, num_games=1000)\n",
        "print(f\"  MC wins: {w1}, Optimal wins: {w2}\")\n",
        "print(f\"  MC win rate: {w1/10:.1f}%\")\n",
        "\n",
        "print(\"\\nTest 2: Optimal Player I vs MC Player II\")\n",
        "w1, w2 = evaluate(game, optimal_agent, agent2, num_games=1000)\n",
        "print(f\"  Optimal wins: {w1}, MC wins: {w2}\")\n",
        "print(f\"  MC win rate: {w2/10:.1f}%\")\n",
        "\n",
        "print(\"\\nTest 3: MC vs MC (self-play)\")\n",
        "w1, w2 = evaluate(game, agent1, agent2, num_games=1000)\n",
        "print(f\"  Player I wins: {w1}, Player II wins: {w2}\")\n",
        "print(f\"  Player I win rate: {w1/10:.1f}%\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Note: With k=3, N=10, optimal Player I should always win.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Watching a Sample Game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_game_verbose(game: AdditionGame, agent1, agent2, name1: str, name2: str, show_q: bool = True):\n",
        "    \"\"\"Play a game with detailed output.\"\"\"\n",
        "    state = game.reset()\n",
        "    agents = [agent1, agent2]\n",
        "    names = [name1, name2]\n",
        "    \n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ADDITION GAME: k={game.k}, N={game.N}\")\n",
        "    print(f\"{name1} (Player I) vs {name2} (Player II)\")\n",
        "    print(f\"First to make sum exceed {game.N} loses!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    move = 1\n",
        "    while not game.done:\n",
        "        player = state.current_player\n",
        "        agent = agents[player]\n",
        "        name = names[player]\n",
        "        \n",
        "        action = agent.select_action(state, game.get_valid_actions(), training=False)\n",
        "        \n",
        "        # Show Q-values if agent is Monte Carlo\n",
        "        if show_q and hasattr(agent, 'Q'):\n",
        "            q_values = agent.Q[state.current_sum]\n",
        "            q_str = \", \".join([f\"Q({a})={q_values[a]:.2f}\" for a in range(1, game.k + 1)])\n",
        "            print(f\"Move {move}: {name} [{q_str}] -> chooses {action}\")\n",
        "        else:\n",
        "            print(f\"Move {move}: {name} chooses {action}\")\n",
        "        \n",
        "        print(f\"         Sum: {state.current_sum} -> {state.current_sum + action}\")\n",
        "        \n",
        "        state, _, _, done = game.step(action)\n",
        "        \n",
        "        if done:\n",
        "            print(f\"\\n         *** SUM ({state.current_sum}) EXCEEDS {game.N}! ***\")\n",
        "            print(f\"\\n  WINNER: {names[game.winner]}\")\n",
        "        else:\n",
        "            print()\n",
        "        \n",
        "        move += 1\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"History: {game.history}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "print(\"Game 1: MC agents playing each other\\n\")\n",
        "play_game_verbose(game, agent1, agent2, \"MC-I\", \"MC-II\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Game 2: MC Player I vs Optimal Player II\\n\")\n",
        "play_game_verbose(game, agent1, optimal_agent, \"MC-I\", \"Optimal-II\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "We implemented **Monte Carlo** reinforcement learning agents for Blackwell's Addition game. Key aspects:\n",
        "\n",
        "1. **Learning from Complete Episodes**: MC methods wait until the end of each game to update Q-values, using the actual return rather than bootstrapped estimates.\n",
        "\n",
        "2. **Sample Average Updates**: Q(s,a) converges to the true expected return as the number of visits to (s,a) increases.\n",
        "\n",
        "3. **No Bias from Bootstrapping**: Unlike TD methods, MC estimates are unbiased (given enough samples), though they have higher variance.\n",
        "\n",
        "### Monte Carlo vs Q-Learning\n",
        "\n",
        "| Aspect | Monte Carlo | Q-Learning (TD) |\n",
        "|--------|-------------|-----------------|\n",
        "| Updates | After episode ends | After each step |\n",
        "| Bootstrapping | No (uses actual returns) | Yes (uses Q estimates) |\n",
        "| Bias | Unbiased | Biased (but consistent) |\n",
        "| Variance | Higher | Lower |\n",
        "| Data efficiency | Lower | Higher |\n",
        "| Works for continuing tasks | No | Yes |\n",
        "\n",
        "### Why MC Works Well for This Game\n",
        "\n",
        "1. **Short Episodes**: Addition games are relatively short, so waiting until the end isn't costly\n",
        "2. **Clear Returns**: The reward structure (+1/-1 at end) makes returns unambiguous\n",
        "3. **No Intermediate Rewards**: All information comes at episode end anyway\n",
        "4. **Finite State Space**: Visit counts converge to meaningful statistics\n",
        "\n",
        "### Connections to Blackwell's Work\n",
        "\n",
        "Monte Carlo methods have deep connections to game theory:\n",
        "- **Empirical Play**: MC learning is like players empirically learning from repeated games\n",
        "- **Minimax Convergence**: With enough exploration, MC finds the minimax strategy\n",
        "- **Blackwell Approachability**: MC's averaging relates to Blackwell's approachability theorem\n",
        "\n",
        "### Extensions\n",
        "\n",
        "- **Monte Carlo Tree Search (MCTS)**: Combine MC sampling with tree search (used in AlphaGo)\n",
        "- **Importance Sampling**: Use off-policy MC with behavior policies\n",
        "- **On-Policy MC Control**: Use epsilon-soft policies throughout\n",
        "- **Variance Reduction**: Use control variates or baseline subtraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_monte_carlo_agents(\n",
        "    game: AdditionGame,\n",
        "    agent1: MonteCarloAgent,\n",
        "    agent2: MonteCarloAgent,\n",
        "    num_episodes: int = 50000,\n",
        "    log_interval: int = 1000\n",
        ") -> dict:\n",
        "    \"\"\"Train two Monte Carlo agents through self-play.\"\"\"\n",
        "    agents = [agent1, agent2]\n",
        "    stats = {\n",
        "        'episodes': [],\n",
        "        'agent1_win_rate': [],\n",
        "        'agent2_win_rate': [],\n",
        "        'epsilon': [],\n",
        "        'avg_game_length': [],\n",
        "        'total_visits': []\n",
        "    }\n",
        "    \n",
        "    game_lengths = []\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
        "        state = game.reset()\n",
        "        \n",
        "        # Store episode history for each agent: list of (state_key, action)\n",
        "        episode_history = [[], []]\n",
        "        \n",
        "        # Play the game\n",
        "        while not game.done:\n",
        "            current_player = state.current_player\n",
        "            agent = agents[current_player]\n",
        "            \n",
        "            state_key = agent.get_state_key(state)\n",
        "            valid_actions = game.get_valid_actions()\n",
        "            action = agent.select_action(state, valid_actions, training=True)\n",
        "            \n",
        "            # Record state-action pair\n",
        "            episode_history[current_player].append((state_key, action))\n",
        "            \n",
        "            state, reward_p1, reward_p2, done = game.step(action)\n",
        "        \n",
        "        # Episode complete - update both agents\n",
        "        rewards = [reward_p1, reward_p2]\n",
        "        game_lengths.append(len(game.history))\n",
        "        \n",
        "        for player_id in [0, 1]:\n",
        "            agent = agents[player_id]\n",
        "            reward = rewards[player_id]\n",
        "            \n",
        "            # Update Q-values from the episode\n",
        "            agent.update_from_episode(episode_history[player_id], reward)\n",
        "            agent.record_result(game.winner == player_id)\n",
        "        \n",
        "        # Decay epsilon for both agents\n",
        "        agent1.decay_epsilon()\n",
        "        agent2.decay_epsilon()\n",
        "        \n",
        "        # Log statistics\n",
        "        if (episode + 1) % log_interval == 0:\n",
        "            stats['episodes'].append(episode + 1)\n",
        "            stats['agent1_win_rate'].append(agent1.wins / max(1, agent1.games) * 100)\n",
        "            stats['agent2_win_rate'].append(agent2.wins / max(1, agent2.games) * 100)\n",
        "            stats['epsilon'].append(agent1.epsilon)\n",
        "            stats['avg_game_length'].append(np.mean(game_lengths[-log_interval:]))\n",
        "            \n",
        "            # Count total visits\n",
        "            total = sum(sum(agent1.N[s].values()) for s in agent1.N)\n",
        "            stats['total_visits'].append(total)\n",
        "            \n",
        "            # Reset counters\n",
        "            agent1.wins = agent1.losses = agent1.games = 0\n",
        "            agent2.wins = agent2.losses = agent2.games = 0\n",
        "    \n",
        "    return stats\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
